<!doctype html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport"
          content="width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <link rel="icon" type="image/x-icon" href="/assets/favicon.ico">
    <title>天津大学-深度强化学习实验室</title>

    
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'pre'],
        inlineMath: [['$','$']]
        }
    });

    </script>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
            type="text/javascript"></script>


    
    <link rel="stylesheet"
          href="https://fonts.loli.net/css?family=Manrope">
    <link rel="stylesheet"
          href="https://fonts.loli.net/css?family=Merriweather:900,900italic,300,300italic">
    <link href="https://fonts.loli.net/icon?family=Material+Icons" rel="stylesheet">
    <link rel="stylesheet"
          href="https://fonts.loli.net/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@20..48,100..700,0..1,-50..200"/>
    <link href="https://fonts.loli.net/css?family=Lato:900,300" rel="stylesheet" type="text/css">
    <link rel="stylesheet"
          href="https://fonts.loli.net/css?family=Playfair+Display:400,900">

    <link rel="stylesheet"
          href="https://fonts.loli.net/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@48,400,0,0"/>
    <style>
        body {
            cursor: default;
            padding: 0;
            margin: 0;
            background-color: #f0f0f0;
        }
    </style>
    <link rel="stylesheet" href="/assets/css/header.css">
    
        
            <link rel="stylesheet" href="/assets/css/main-poster.css">
        
            <link rel="stylesheet" href="/assets/css/news.css">
        
            <link rel="stylesheet" href="/assets/css/research.css">
        
    
    <link rel="stylesheet" href="/assets/css/footer.css">
    

    

</head>
<body>
<script>
  // Wait for the DOM to fully load
  document.addEventListener("DOMContentLoaded", () => {
    // JavaScript code to select all <a> tags with class != "inner"
    const anchorTagsWithoutInnerClass = document.querySelectorAll('a:not(.inner)');

    // Loop through the selected anchor tags and add the target="_blank" attribute
    anchorTagsWithoutInnerClass.forEach(anchor => {
      anchor.setAttribute('target', '_blank');
    });
  });

</script>
<section id="header">
    <header>
        <div class="logo">
            <img src="/assets/image/logo.png" alt=""/>
        </div>
        <div class="action">
            <div class="tab">
                <a class="tab-item inner" href="/">HOME</a>
                <a class="tab-item inner" href="/news">NEWS</a>
                <a class="tab-item inner" href="/research">RESEARCH</a>
                <!--                <a class="tab-item inner" href="/publications">PUBLICATIONS</a>-->
                <div class="dropdown-menu">
                    <span class="text">PUBLICATIONS</span>
                    <div class="content">
                        <div onclick="window.location.href ='/conference'" class="tab-item item">
                            CONFERENCE
                        </div>
                        <div class="divider"></div>
                        <div onclick="window.location.href ='/journal'" class="tab-item item">
                            JOURNAL
                        </div>
                    </div>
                </div>
                <!--<a class="tab-item inner" href="/software">SOFTWARE</a>-->
                <a class="tab-item inner" href="/team">TEAM</a>
                <!--                <a class="tab-item inner" href="/ibbb">IBBB</a>-->
                <!--                <a class="tab-item inner" href="/events">EVENTS</a>-->
                <a class="tab-item inner" href="/contact">CONTACT</a>
            </div>
            <div style="display: none" class="contact" onclick="window.location.href='/contact'">
                <span>Contact Us</span>
            </div>
            <div class="thin-menu">
                <input type="checkbox" id="menu-toggle">
                <label for="menu-toggle" class="menu-btn">
                    <i class="material-icons">menu</i>
                </label>
                <ul class="menu">
                    <li><a class="inner" href="/">HOME</a></li>
                    <li><a class="inner" href="/news">NEWS</a></li>
                    <li><a class="inner" href="/research">RESEARCH</a></li>
                    <!--                    <li><a class="inner" href="/publications">PUBLICATIONS</a></li>-->
                    <li><a class="inner" href="/conference">CONFERENCE</a></li>
                    <li><a class="inner" href="/journal">JOURNAL</a></li>
                    <!--<li><a class="inner" href="/software">SOFTWARE</a></li>-->
                    <li><a class="inner" href="/team">TEAM</a></li>
                    <!--                    <li><a class="inner" href="/ibbb">IBBB</a></li>-->
                    <!--                    <li><a class="inner" href="/events">EVENTS</a></li>-->
                    <li><a class="inner" href="/contact">CONTACT</a></li>
                </ul>
            </div>
        </div>
    </header>
</section>
<script>
  // Variables to store initial mouse position and banner position
  let initialMouseX, initialMouseY, initialBannerX, initialBannerY;

  // Function to create the banner element
  function createBanner() {
    let banner = document.createElement("div");
    banner.id = "recruitmentBanner";
    banner.style.position = "fixed";
    if (document.body.clientWidth < 768) {
      banner.style.top = "60vh"; // Update the top offset to 20vh
    } else {
      banner.style.top = "20vh"; // Update the top offset to 20vh
    }
    banner.style.right = "20px"; // Update the right offset to 20px
    banner.style.backgroundColor = "#EBF3F5";
    banner.style.color = "#333";
    banner.style.padding = "15px"; // Update the padding to 15px
    banner.style.width = "px";
    banner.style.borderRadius = "5px";
    banner.style.zIndex = "999";
    banner.style.cursor = "move"; // Set cursor to "move" to indicate draggability
    banner.style.boxShadow = "0px 2px 5px rgba(0, 0, 0, 0.2)";
    banner.style.opacity = "0"; // Initially set to transparent
    banner.style.transition = "opacity 0.3s ease-in-out"; // Add the transition effect here

    let closeButton = document.createElement("span");
    closeButton.style.position = "absolute";
    closeButton.style.top = "5px";
    closeButton.style.right = "5px";
    closeButton.style.fontSize = "16px";
    closeButton.style.cursor = "pointer";
    closeButton.textContent = "×";
    closeButton.addEventListener("click", hideBanner);

    let bannerText = document.createElement("p");
    bannerText.innerHTML = ``;
    bannerText.style.margin = "0"; // Remove margin from the <p> element

    banner.appendChild(closeButton);
    banner.appendChild(bannerText);

    // Make the banner draggable
    banner.addEventListener("mousedown", startDragging);
    banner.addEventListener("touchstart", startDragging);

    return banner;
  }

  function convertTopToPx(topValue) {
    // Check if the topValue ends with 'vh'
    if (topValue.endsWith('vh')) {
      const vhValue = parseFloat(topValue); // Parse the numeric value of vh
      // Convert vh to px
      return (vhValue * window.innerHeight) / 100;
    }

    // If the topValue does not end with 'vh', assume it's already in 'px'
    return parseFloat(topValue);
  }

  function rightToLeft(rightValue) {
    rightValue = parseInt(rightValue)
    const containerWidth = window.innerWidth // Get the container's width (the body width in this case)
    return containerWidth - rightValue;
  }

  function leftToRight(leftValue) {
    leftValue = parseInt(leftValue)
    const containerWidth = window.innerWidth // Get the container's width (the body width in this case)
    return containerWidth - leftValue;
  }

  // Function to start dragging the banner
  function startDragging(event) {
    console.log(event)
    if (event.type === "mousedown") {
      // For mouse events
      initialMouseX = event.clientX;
      initialMouseY = event.clientY;
    } else if (event.type === "touchstart") {
      // For touch events
      initialMouseX = event.targetTouches[0].clientX;
      initialMouseY = event.targetTouches[0].clientY;
    }
    const banner = document.getElementById("recruitmentBanner");
    initialBannerX = rightToLeft(banner.style.right) || 0;
    initialBannerY = convertTopToPx(banner.style.top) || 0;

    document.addEventListener("mousemove", dragBanner);
    document.addEventListener("mouseup", stopDragging);
    document.addEventListener("touchmove", dragBanner, {passive: false});
    document.addEventListener("touchend", stopDragging);
  }

  // Function to drag the banner
  function dragBanner(event) {
    event.preventDefault()
    let currentMouseX, currentMouseY;
    if (event.type === "mousemove") {
      // For mouse events
      currentMouseX = event.clientX;
      currentMouseY = event.clientY;
    } else if (event.type === "touchmove") {
      // For touch events
      currentMouseX = event.targetTouches[0].clientX;
      currentMouseY = event.targetTouches[0].clientY;
    }

    const offsetX = currentMouseX - initialMouseX;
    const offsetY = currentMouseY - initialMouseY;

    const banner = document.getElementById("recruitmentBanner");
    banner.style.right = `${leftToRight(initialBannerX + offsetX)}px`;
    banner.style.top = `${initialBannerY + offsetY}px`;
  }


  // Function to stop dragging the banner
  function stopDragging() {
    document.removeEventListener("mousemove", dragBanner);
    document.removeEventListener("mouseup", stopDragging);

    document.removeEventListener("touchmove", dragBanner);
    document.removeEventListener("touchend", stopDragging);
  }

  // Function to show the banner with animation
  function showBanner() {
    var banner = document.getElementById("recruitmentBanner");
    if (!banner) {
      banner = createBanner();
      document.body.appendChild(banner);
    }
    banner.style.opacity = "1"; // Change opacity to 1 to trigger the transition
  }

  // Function to hide the banner with animation
  function hideBanner() {
    let banner = document.getElementById("recruitmentBanner");
    if (banner && banner.style.opacity === "1") {
      banner.style.opacity = "0"; // Change opacity to 0 to trigger the transition
    }
    setTimeout(() => {
      banner.style.display = "none";
    }, 250)
  }

  
</script>

<script>
  console.log("get")
  let currentIndex = 0;
  let timer;
  let slides;

  function showSlide(index) {
    slides.forEach(slide => {
      slide.style.transform = `translateX(${-index * 100}%)`;
    });
  }

  function nextSlide() {
    currentIndex = (currentIndex + 1) % slides.length;
    console.log(slides.length);
    showSlide(currentIndex);
  }

  function startTimer() {
    timer = setInterval(nextSlide, 2000); // 切换间隔为 3 秒
  }

  function stopTimer() {
    clearInterval(timer);
  }

  function begin() {
    slides = document.querySelectorAll('.carousel-slide');
    showSlide(currentIndex);
    startTimer();
    slides.forEach(slide => {
      slide.addEventListener('mouseenter', stopTimer);
      slide.addEventListener('mouseleave', startTimer);
    });
  }

  window.addEventListener('DOMContentLoaded', () => {
    begin();
  });
</script>

<section id="poster">
    <div class="slogan">
        <div class="slogan-area">
            <div class="slogan-title">
                <span>Tianjin University</span>
                <div style="white-space: nowrap">
                    <span class="strong">
                        Deep Reinforcement
                    </span>
                </div>
                <div>
                    <span class="strong">Learning</span>
                    Laboratory
                </div>
            </div>
            <div class="slogan-content balance-warp">
                Our lab has several Ph.D. and Master positions. If you are interested in our research, please send us your CV. (jianye.hao@tju.edu.cn / yanzheng@tju.edu.cn) <br><br> 实验室长期接受优秀同学交流学习，攻读硕士/博士学位的同学加入。同时欢迎感兴趣学部（院）夏令营活动的同学进行邮件联系！
            </div>
        </div>
    </div>
    <div class="poster-background">
        <div class="poster-background-box">
            <div class="poster-image carousel-container">
                
                    <div class="carousel-slide" style="background-image: url('assets/image/index/metadiffuer.png')">
                        <div class="carousel-caption">
                            <a href="https://arxiv.org/abs/2305.19923" class="carousel-link">
                                <p>MetaDiffuser: Diffusion Model as Conditional Planner for Offline Meta-RL</p>
                            </a>
                        </div>
                    </div>
                
                    <div class="carousel-slide" style="background-image: url('assets/image/index/re2.png')">
                        <div class="carousel-caption">
                            <a href="https://openreview.net/pdf?id=FYZCHEtt6H0" class="carousel-link">
                                <p>ERL-Re2: Efficient Evolutionary Reinforcement Learning with Shared State Representation and Individual Policy Representation</p>
                            </a>
                        </div>
                    </div>
                
                    <div class="carousel-slide" style="background-image: url('assets/image/index/api.png')">
                        <div class="carousel-caption">
                            <a href="https://openreview.net/pdf?id=OxNQXyZK-K8" class="carousel-link">
                                <p>Boosting Multiagent Reinforcement Learning via Permutation Invariant and Permutation Equivariant Networks</p>
                            </a>
                        </div>
                    </div>
                
            </div>
            
            
        </div>
    </div>
</section>
<section id="news">
    <div class="news-title">
        <span>News</span>
    </div>
    <div class="news-content">
        
        
            <div class="news-item">
                <div class="news-icon">
                    
                        <i class="material-symbols-outlined" style="font-size: 3rem">lightbulb</i>
                    
                </div>
                <div class="news-text">
                    <div class="news-text-title">
                        May  5, 2023 - Three papers accepted by ICML 2023:
                    </div>
                    <div class="news-text-desc">
                        "ChiPFormer: Transferable Chip Placement via Offline Decision Transformer","MetaDiffuser: Diffusion Model as Conditional Planner for Offline Meta-RL","RACE: Improve Multi-Agent Reinforcement Learning with Representation Asymmetry and Collaborative Evolution"
                    </div>
                </div>
            </div>
            
                <div class="news-divider"></div>
            
        
            <div class="news-item">
                <div class="news-icon">
                    
                        <i class="material-symbols-outlined" style="font-size: 3rem">stars</i>
                    
                </div>
                <div class="news-text">
                    <div class="news-text-title">
                        Jan 21, 2023 - Seven papers accepted by ICLR 2023:
                    </div>
                    <div class="news-text-desc">
                        "ERL-Re2: Efficient Evolutionary Reinforcement Learning with Shared State Representation and Individual Policy Representation","Breaking the Curse of Dimensionality in Multiagent State Space: A Unified Agent Permutation Framework","EUCLID: Towards Efficient Unsupervised Reinforcement Learning with Multi-choice Dynamics Model","Learnable Behavior Control: Breaking Atari Human World Records via Sample-Efficient Behavior Selection","DAG Matters! GFlowNets Enhanced Explainer for Graph Neural Networks","CFlowNets: Continuous control with Generative Flow Network","Out-of-distribution Detection with Implicit Outlier Transformation"
                    </div>
                </div>
            </div>
            
                <div class="news-divider"></div>
            
        
            <div class="news-item">
                <div class="news-icon">
                    
                        <i class="material-symbols-outlined" style="font-size: 3rem">rocket_launch</i>
                    
                </div>
                <div class="news-text">
                    <div class="news-text-title">
                        Nov 25, 2022 - Four papers accepted by AAAI 2023:
                    </div>
                    <div class="news-text-desc">
                        "SplitNet: A Reinforcement Learning based Sequence Splitting Method for the MinMax Multiple Travelling Salesman Problem","Neighbor Auto-Grouping Graph Neural Networks for Handover Parameter Configuration in Cellular Network","Structure Aware Incremental Learning with Personalized Imitation Weights for Recommender Systems","Models as Agents: Optimizing Muti-Step Predictions of interactive Local Models in Model-Based Multi-Agent Reinforcement Learning"
                    </div>
                </div>
            </div>
            
                <div class="news-divider"></div>
            
        
            <div class="news-item">
                <div class="news-icon">
                    
                        <i class="material-symbols-outlined" style="font-size: 3rem">campaign</i>
                    
                </div>
                <div class="news-text">
                    <div class="news-text-title">
                        Sep 15, 2022 - Seven papers accepted by NeurIPS 2022:
                    </div>
                    <div class="news-text-desc">
                        "Multiagent Q-learning with Sub-Team Coordination","Transformer-based Working Memory for Multiagent Reinforcement Learning with Action Parsing","GALOIS: Boosting Deep Reinforcement Learning via Generalizable Logic Synthesis","Versatile Multi-stage Graph Neural Network for Circuit Representation","The Policy-gradient Placement and Generative Routing Neural Networks for Chip Design","DOMINO: Decomposed Mutual Information Optimization for Generalized Context in Meta-Reinforcement Learning","Plan To Predict: Learning an Uncertainty-Foreseeing Model For Model-Based Reinforcement Learning"
                    </div>
                </div>
            </div>
            
        
    </div>
    <div class="news-more" onclick="window.location.href='news'">
        READ MORE
    </div>
</section>
<section id="research">

    <script>

      let go = (event) => {
        if (!event.target.classList.contains("banner")) {
          window.open(event.currentTarget.dataset.url)
        }
      }

      window.addEventListener("DOMContentLoaded", () => {
        document.querySelectorAll(".research-card").forEach(input => {
          input.addEventListener("click", go);
        });
      })

    </script>

    <div class="research-title">
        Recent Research
    </div>
    <div class="research-cards">
        
        
            <div class="research-card" data-url="/2023/10/21/PeVFA.html">
                <div class="research-image"
                     style="background-image: url('/assets/image/research/PeVFA/Untitled.png');
                     
                             background-size:contain;
                     ">
                </div>
                <div class="research-body">
                    <div class="research-card-title">
                        What About Inputting Policy in Value Function: Policy Representation and Policy-extended Value Function Approximator
                    </div>
                    <div class="research-card-desc">
                        2023-10-21: We study Policy-extended Value Function Approximator (PeVFA) in Reinforcement Learning (RL), which extends conventional value function approximator (VFA) to take as input not only the state (and action) but also an explicit policy representation. Such an extension enables PeVFA to preserve values of multiple policies at the same time and brings an appealing characteristic, i.e., *value generalization among policies*.
                    </div>
                    <div class="research-card-action">
                        
                            <a target="_blank" href="https://arxiv.org/pdf/2010.09536.pdf" class="banner">PDF</a>
                        
                            <a target="_blank" href="https://github.com/TJU-DRL-LAB/self-supervised-rl/tree/main/RL_with_Policy_Representation/Policy-based_RL_with_PeVFA/PPO-PeVFA" class="banner">Code</a>
                        
                    </div>
                </div>
            </div>
        
            <div class="research-card" data-url="/2023/09/10/Re2.html">
                <div class="research-image"
                     style="background-image: url('/assets/image/index/re2.png');
                     
                             background-size:contain;
                     ">
                </div>
                <div class="research-body">
                    <div class="research-card-title">
                        ERL-Re2: Efficient Evolutionary Reinforcement Learning with Shared State Representation and Individual Policy Representation
                    </div>
                    <div class="research-card-desc">
                        2023-09-10: Deep Reinforcement Learning (Deep RL) and Evolutionary Algorithms (EA) are two major paradigms of policy optimization with distinct learning principles, i.e., gradient-based v.s. gradient-free. An appealing research direction is integrating Deep RL and EA to devise new methods by fusing their complementary advantages. However, existing works on combining Deep RL and EA have two common drawbacks: 1) the RL agent and EA agents learn their policies individually, neglecting efficient sharing of useful common knowledge; 2) parameter-level policy optimization guarantees no semantic level of behavior evolution for the EA side. In this paper, we propose Evolutionary Reinforcement Learning with Two-scale State Representation and Policy Representation (ERL-Re^2), a novel solution to the aforementioned two drawbacks. The key idea of ERL-Re^2 is two-scale representation: all EA and RL policies share the same nonlinear state representation while maintaining individual} linear policy representations. The state representation conveys expressive common features of the environment learned by all the agents collectively; the linear policy representation provides a favorable space for efficient policy optimization, where novel behavior-level crossover and mutation operations can be performed. Moreover, the linear policy representation allows convenient generalization of policy fitness with the help of the Policy-extended Value Function Approximator (PeVFA), further improving the sample efficiency of fitness estimation. The experiments on a range of continuous control tasks show that ERL-Re^2 consistently outperforms advanced baselines and achieves the State Of The Art (SOTA). 
                    </div>
                    <div class="research-card-action">
                        
                            <a target="_blank" href="https://arxiv.org/abs/2210.17375" class="banner">PDF</a>
                        
                            <a target="_blank" href="https://github.com/yeshenpy/ERL-Re2" class="banner">Code</a>
                        
                    </div>
                </div>
            </div>
        
            <div class="research-card" data-url="/2023/09/09/API.html">
                <div class="research-image"
                     style="background-image: url('/assets/image/research/iclr23_pi_pe.png');
                     
                             background-size:contain;
                     ">
                </div>
                <div class="research-body">
                    <div class="research-card-title">
                        Boosting Multiagent Reinforcement Learning via Permutation Invariant and Permutation Equivariant Networks
                    </div>
                    <div class="research-card-desc">
                        2023-09-09: The state space in Multiagent Reinforcement Learning (MARL) grows exponentially with the agent number. Such a curse of dimensionality results in poor scalability and low sample efficiency, inhibiting MARL for decades. To break this curse, we propose a unified agent permutation framework that exploits the permutation invariance (PI) and permutation equivariance (PE) inductive biases to reduce the multiagent state space. Our insight is that permuting the order of entities in the factored multiagent state space does not change the information.
                    </div>
                    <div class="research-card-action">
                        
                            <a target="_blank" href="https://openreview.net/pdf?id=OxNQXyZK-K8" class="banner">PDF</a>
                        
                            <a target="_blank" href="https://github.com/tjuHaoXiaotian/pymarl3" class="banner">Code</a>
                        
                    </div>
                </div>
            </div>
        
            <div class="research-card" data-url="/2023/08/01/MetaDiffuser.html">
                <div class="research-image"
                     style="background-image: url('/assets/image/research/metadiffuer.png');
                     
                             background-size:contain;
                     ">
                </div>
                <div class="research-body">
                    <div class="research-card-title">
                        MetaDiffuser: Diffusion Model as Conditional Planner for Offline Meta-RL
                    </div>
                    <div class="research-card-desc">
                        2023-08-01: Recently, diffusion model shines as a promising backbone for the sequence modeling paradigm in offline reinforcement learning. However, these works mostly lack the generalization ability across tasks with reward or dynamics change. To tackle this challenge, in this paper we propose a task-oriented conditioned diffusion planner for offline meta-RL(MetaDiffuser), which considers the generalization problem as conditional trajectory generation task with contextual representation. The key is to learn a context conditioned diffusion model which can generate task-oriented trajectories for planning across diverse tasks. To enhance the dynamics consistency of the generated trajectories while encouraging trajectories to achieve high returns, we further design a dual-guided module in the sampling process of the diffusion model. The proposed framework enjoys the robustness to the quality of collected warm-start data from the testing task and the flexibility to incorporate with different task representation method. The experiment results on MuJoCo benchmarks show that MetaDiffuser outperforms other strong offline meta-RL baselines, demonstrating the outstanding conditional generation ability of diffusion architecture.
                    </div>
                    <div class="research-card-action">
                        
                            <a target="_blank" href="http://proceedings.mlr.press/v202/ni23a/ni23a.pdf" class="banner">PDF</a>
                        
                            <a target="_blank" href="https://metadiffuser.github.io" class="banner">Site</a>
                        
                    </div>
                </div>
            </div>
        
    </div>
    <div class="research-more" onclick="window.location.href='research'">
        READ MORE
    </div>
</section>

<section id="footer">
    <div class="footer-body">
        <div class="footer-body-holder">
            <div class="logos">
                <div class="logo"
                     style="background-image: url('/assets/image/logo-white.png');"></div>
                <div class="text">
                <span class="strong">
                Deep Reinforcement Learning
                </span>
                    Laboratory
                </div>
            </div>
            <div class="footer-divider"></div>
            <div class="footer-actions">
                <div class="action-item extra-margin">
                    <div class="action-title">
                        Contact Us
                    </div>
                    <div class="action-body action-item">
                        <table style="border-spacing: 14px;border-collapse: collapse;user-select: text">
                            <tr>
                                <td style="width: 120px;vertical-align: top">
                                    <i class="material-icons">location_on</i>
                                </td>
                                <td style="width: 550px;vertical-align: top">
                                    天津市津南区海河教育园区雅观路135号天津大学软件学院55号楼A区
                                </td>
                            </tr>
                            <tr>
                                <td style="display: flex; align-items: center"><i class="material-icons">mail</i></td>
                                <td>
                                    <a class="hover-blue" href="mailto:jianye.hao@tju.edu.cn"
                                       style="color: white;text-decoration: underline;">
                                        jianye.hao@tju.edu.cn
                                    </a>
                                </td>
                            </tr>
                            <tr style="display: none">
                                <td>Phone:</td>
                                <td>(+22) 123 - 4567 - 900</td>
                            </tr>
                        </table>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="copyright">

        <div>
            <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            <span id="busuanzi_container_site_pv">本站总访问量<span id="busuanzi_value_site_pv"></span>次</span>
        </div>
        <div>
            <span>©️ TIANJIN UNIVERSITY-DEEP REINFORCEMENT LEARNING LAB</span>
            <span style="margin-left: 25px;">ALL RIGHTS RESERVED</span><br/>
        </div>
    </div>
</section>
</body>
</html>
