<!doctype html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport"
          content="width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <link rel="icon" type="image/x-icon" href="/assets/favicon.ico">
    <title>What About Inputting Policy in Value Function: Policy Representation and Policy-extended Value Function Approximator</title>

    
<!--    <script type="text/x-mathjax-config">-->
<!--    MathJax.Hub.Config({-->
<!--        tex2jax: {-->
<!--        skipTags: ['script', 'noscript', 'style', 'pre'],-->
<!--        inlineMath: [['$','$']]-->
<!--        }-->
<!--    });-->

<!--    </script>-->

    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [
                ['$', '$'],
                ['\\(', '\\)']
            ],
            displayMath: [
                ['$$', '$$'],
                ["\\[", "\\]"]
            ],
            processEscapes: true
        },
        TeX: {
            extensions: ["AMSmath.js", "AMSsymbols.js"],
            equationNumbers: {
                autoNumber: ["AMS"],
                useLabelIds: true
            },
            Macros: {
                hfill: "{}"
					}
        },
        "HTML-CSS": {
            linebreaks: {
                automatic: true
            },
            availableFonts: ["TeX"],
            scale: 110
        },
        SVG: {
            linebreaks: {
                automatic: true
            }
        }
    });
	</script>

    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
            type="text/javascript"></script>


    
    <link rel="stylesheet"
          href="https://fonts.loli.net/css?family=Manrope">
    <link rel="stylesheet"
          href="https://fonts.loli.net/css?family=Merriweather:900,900italic,300,300italic">
    <link href="https://fonts.loli.net/icon?family=Material+Icons" rel="stylesheet">
    <link rel="stylesheet"
          href="https://fonts.loli.net/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@20..48,100..700,0..1,-50..200"/>
    <link href="https://fonts.loli.net/css?family=Lato:900,300" rel="stylesheet" type="text/css">
    <link rel="stylesheet"
          href="https://fonts.loli.net/css?family=Playfair+Display:400,900">

    <link rel="stylesheet"
          href="https://fonts.loli.net/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@48,400,0,0"/>
    <style>
        body {
            cursor: default;
            padding: 0;
            margin: 0;
            background-color: #f0f0f0;
        }
    </style>
    <link rel="stylesheet" href="/assets/css/header.css">
    
    <link rel="stylesheet" href="/assets/css/footer.css">
    
        <link rel="stylesheet" href="/assets/css/post.css">
    

    

</head>
<body>
<script>
  // Wait for the DOM to fully load
  document.addEventListener("DOMContentLoaded", () => {
    // JavaScript code to select all <a> tags with class != "inner"
    const anchorTagsWithoutInnerClass = document.querySelectorAll('a:not(.inner)');

    // Loop through the selected anchor tags and add the target="_blank" attribute
    anchorTagsWithoutInnerClass.forEach(anchor => {
      anchor.setAttribute('target', '_blank');
    });
  });

</script>
<section id="header">
    <header>
        <div class="logo">
            <img src="/assets/image/logo.png" alt=""/>
        </div>
        <div class="action">
            <div class="tab">
                <a class="tab-item inner" href="/">HOME</a>
                <a class="tab-item inner" href="/news">NEWS</a>
                <a class="tab-item inner" href="/research">RESEARCH</a>
                <!--                <a class="tab-item inner" href="/publications">PUBLICATIONS</a>-->
                <div class="dropdown-menu">
                    <span class="text">PUBLICATIONS</span>
                    <div class="content">
                        <div onclick="window.location.href ='/conference'" class="tab-item item">
                            CONFERENCE
                        </div>
                        <div class="divider"></div>
                        <div onclick="window.location.href ='/journal'" class="tab-item item">
                            JOURNAL
                        </div>
                    </div>
                </div>
                <!--<a class="tab-item inner" href="/software">SOFTWARE</a>-->
                <a class="tab-item inner" href="/team">TEAM</a>
                <!--                <a class="tab-item inner" href="/ibbb">IBBB</a>-->
                <!--                <a class="tab-item inner" href="/events">EVENTS</a>-->
                <a class="tab-item inner" href="/contact">CONTACT</a>
            </div>
            <div style="display: none" class="contact" onclick="window.location.href='/contact'">
                <span>Contact Us</span>
            </div>
            <div class="thin-menu">
                <input type="checkbox" id="menu-toggle">
                <label for="menu-toggle" class="menu-btn">
                    <i class="material-icons">menu</i>
                </label>
                <ul class="menu">
                    <li><a class="inner" href="/">HOME</a></li>
                    <li><a class="inner" href="/news">NEWS</a></li>
                    <li><a class="inner" href="/research">RESEARCH</a></li>
                    <!--                    <li><a class="inner" href="/publications">PUBLICATIONS</a></li>-->
                    <li><a class="inner" href="/conference">CONFERENCE</a></li>
                    <li><a class="inner" href="/journal">JOURNAL</a></li>
                    <!--<li><a class="inner" href="/software">SOFTWARE</a></li>-->
                    <li><a class="inner" href="/team">TEAM</a></li>
                    <!--                    <li><a class="inner" href="/ibbb">IBBB</a></li>-->
                    <!--                    <li><a class="inner" href="/events">EVENTS</a></li>-->
                    <li><a class="inner" href="/contact">CONTACT</a></li>
                </ul>
            </div>
        </div>
    </header>
</section>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$']],
      displayMath: [             // start/end delimiter pairs for display math
        ['$$', '$$'],
        ['\\[', '\\]']
      ],
      processEscapes: true
    }
  };
</script>
<section id="post">
    <div class="post-body">
        <div class="post-title">What About Inputting Policy in Value Function: Policy Representation and Policy-extended Value Function Approximator</div>
        <div class="post-subtitle">
            <span style="margin-right: 10px;color: #0076df;">Hongyao Tang</span>
            October 21, 2023 </div>
        <!-- for links -->
        <div class="post-links">
            
                <a href="https://arxiv.org/pdf/2010.09536.pdf" target="_blank" class="banner">
                    
                        
                            <i class="material-icons" style="margin-right: 5px;">picture_as_pdf</i>
                        
                    
                    PDF
                </a>
            
                <a href="https://github.com/TJU-DRL-LAB/self-supervised-rl/tree/main/RL_with_Policy_Representation/Policy-based_RL_with_PeVFA/PPO-PeVFA" target="_blank" class="banner">
                    
                        
                            <img style="height: 1.3rem;margin-right: 5px;"
                                 src="/assets/image/github-mark.svg" alt="">
                        
                    
                    Code
                </a>
            
        </div>
        <div class="post-poster">
            <img style="width: 100%; height: auto"
                 src="/assets/image/research/PeVFA/Untitled.png" alt="poster">
        </div>
        <div style="font-family: 'Merriweather',ui-serif;font-size: 18px; overflow-x: auto;">
            <h1 id="what-about-inputting-policy-in-value-function-policy-representation-and-policy-extended-value-function-approximator">What About Inputting Policy in Value Function: Policy Representation and Policy-extended Value Function Approximator</h1>

<h1 id="abstract">Abstract</h1>

<p>One fundamental element of RL is value function which defines the long-term evaluation of a policy. With function approximation (e.g., deep neural networks), a value function approximator (VFA) is able to approximate the values of a policy under large and continuous state spaces.</p>

<p>We study Policy-extended Value Function Approximator (PeVFA) in Reinforcement Learning (RL), which extends conventional value function approximator (VFA) to take as input not only the state (and action) but also an explicit policy representation. Such an extension enables PeVFA to preserve values of multiple policies at the same time and brings an appealing characteristic, i.e., <em>value generalization among policies</em>.</p>

<p>We formally analyze the value generalization under Generalized Policy Iteration (GPI). From theoretical and empirical lens, we show that generalized value estimates offered by PeVFA
may have lower initial approximation error to true values of successive policies, which is expected to improve consecutive value approximation during GPI. Based on above clues, we introduce a new form of GPI with PeVFA which leverages the value generalization along policy improvement path. Moreover, we propose a representation learning framework for RL policy, providing several approaches to learn effective policy embeddings from policy network parameters or stateaction pairs. In our experiments, we evaluate the efficacy of value generalization offered by PeVFA and policy representation learning in several OpenAI Gym continuous control tasks.</p>

<h1 id="policy-extended-value-function-approximation-pevfa">Policy-extended Value Function Approximation (PeVFA)</h1>

<h3 id="definition-of-pevfa">Definition of PeVFA</h3>

<p>Since the <em>conventional value functions</em> are defined on a single policy, how can a value function estimates the values of <em>multiple policies</em>?</p>

<p>To answer this question, we propose <strong>Policy-extended Value Function Approximator</strong> (<strong>PeVFA</strong>):</p>

\[\mathbb{V}(s,\chi_{\pi}) = \mathbb{E}_{\pi} = \left[ \sum_{t=0}^{\infty} \gamma^{t}r_{t+1} | s_0=s \right], \text{for all }s \in S, \pi \in \Pi.\]

<ul>
  <li>Additionally takes a <em>policy representation</em> <strong>$\chi_{\pi}$</strong> as input (just consider we have one here, and we will discuss how to get $<strong>\chi_{\pi}</strong>$ later)</li>
  <li>Thus, has the ability/capacity to preserve values of multiple policies</li>
  <li>Values are expected to <em>generalize among policies</em> <em>**</em>with FA</li>
</ul>

<h3 id="two-kinds-of-value-generalization-of-pevfa">Two Kinds of Value Generalization of PeVFA</h3>

<p>It is exciting to see the value generalization among policies brought by PeVFA. We consider two kinds of value generalization below, which are also illustrated in the figure below:</p>

<ul>
  <li><em>Global</em>: from learned policies ($\pi \in \Pi_0$) to unlearned ones ($\pi^{\prime} \in \Pi_1$)</li>
  <li><em>Local</em>: from previous policies (${\pi_i}<em>{i=0}^t$) to the successive one ($\pi</em>{t+1}$) along the <em>policy improvement path of GPI (our focus of this paper)</em></li>
</ul>

<p><img src="/assets/image/research/PeVFA/Untitled 1.png" alt="Untitled" /></p>

<h3 id="general-policy-iteration-with-pevfa">General Policy Iteration with PeVFA</h3>

<p>Naturally, the following questions are:</p>

<ul>
  <li><em>Is value generalization necessarily beneficial?</em></li>
  <li><em>If so, how can we utilize it to improve RL?</em></li>
</ul>

<p>For the first question, we formularize the value generalization of PeVFA and analyze the situations where the value generalization can be beneficial to typical RL process. We refer interested readers to our paper for more details.</p>

<p>For the second question, we expect to leverage the value generalization of PeVFA to facilitate RL in a general way. To be concrete, we propose General Policy Iteration with PeVFA (GPI with PeVFA). A general description of RL algorithm under the paradigm of GPI with PeVFA is shown below:</p>

<p><img src="/assets/image/research/PeVFA/Untitled 2.png" alt="Untitled" /></p>

<p>For each iteration, the interaction experiences of current policy and the policy representation are stored in a buffer (line 3-4). At an interval of $M$ iterations, PeVFA is trained via value approximation for previous policies with the stored data and the policy representation model is updated according to the method used (line 5-8). This part is unique to PeVFA for preservation and generalization of knowledge of historical policies. Next, value approximation for current policy is performed with PeVFA (line 9). A key difference here is that the generalized value estimates (i.e., $\mathbb{V}<em>{t-1}(\chi</em>{\pi_t})$) are used as start points. Afterwards, a successive policy is obtained from typical policy improvement (line 10).</p>

<p>Algorithm 1 can be implemented in different ways and we propose an instance implemented based on PPO (Schulman et al. 2017) in our experiments.</p>

<h1 id="a-policy-representation-learning-framework">A Policy Representation Learning Framework</h1>

<p>To derive practical deep RL algorithms, one key point is policy representation, i.e., a low-dimensional embedding of RL policy. Now we come back to the problem of obtaining the policy representation $\chi_{\pi}$ for a typical Deep RL policy $\pi$.</p>

<p>The framework of policy representation learning proposed in our work is shown below. Policy network parameters used for <em>Origin Policy Representation (OPR)</em> or policy state-action pairs used for <em>Surface Policy Representation (SPR)</em> are fed into policy encoder with permutation-invariant (PI) transformations followed by an MLP, producing the representation $\chi_\pi$.</p>

<p>Afterwards, $\chi_{\pi}$ can be trained by gradients from the value approximation loss of PeVFA (i.e., End-to-End), as well as the auxiliary loss of policy recovery or contrastive learning (i.e., InfoNCE) loss.</p>

<p><img src="/assets/image/research/PeVFA/Untitled 3.png" alt="Untitled" /></p>

<h1 id="experiments">Experiments</h1>

<p>According to the paradigm of GPI with PeVFA and the general algorithm shown above, we replace the value network of vanilla PPO with PeVFA and obtain PPO-PeVFA. Experiments are conducted in OpenAI Gym continuous control tasks.</p>

<p>The aggregated performance of considered algorithms for comparative evaluation is shown below.</p>

<p><img src="/assets/image/research/PeVFA/Untitled 4.png" alt="Untitled" /></p>

<h3 id="visual-analysis">Visual Analysis</h3>

<p>The 2D t-SNE visualization of learned policy representation is shown in the figure below. We can observe that policies from different trials are locally continuous and show different modes of embedding trajectories due to random initialization and optimization; while a global evolvement among trials emerges with respect to policy performance.</p>

<p><img src="/assets/image/research/PeVFA/Untitled 5.png" alt="Untitled" /></p>

        </div>
    </div>
</section>

<section id="footer">
    <div class="footer-body">
        <div class="footer-body-holder">
            <div class="logos">
                <div class="logo"
                     style="background-image: url('/assets/image/logo-white.png');"></div>
                <div class="text">
                <span class="strong">
                Deep Reinforcement Learning
                </span>
                    Laboratory
                </div>
            </div>
            <div class="footer-divider"></div>
            <div class="footer-actions">
                <div class="action-item extra-margin">
                    <div class="action-title">
                        Contact Us
                    </div>
                    <div class="action-body action-item">
                        <table style="border-spacing: 14px;border-collapse: collapse;user-select: text">
                            <tr>
                                <td style="width: 120px;vertical-align: top">
                                    <i class="material-icons">location_on</i>
                                </td>
                                <td style="width: 550px;vertical-align: top">
                                    天津市津南区海河教育园区雅观路135号天津大学软件学院55号楼A区
                                </td>
                            </tr>
                            <tr>
                                <td style="display: flex; align-items: center"><i class="material-icons">mail</i></td>
                                <td>
                                    <a class="hover-blue" href="mailto:jianye.hao@tju.edu.cn"
                                       style="color: white;text-decoration: underline;">
                                        jianye.hao@tju.edu.cn
                                    </a>
                                </td>
                            </tr>
                            <tr style="display: none">
                                <td>Phone:</td>
                                <td>(+22) 123 - 4567 - 900</td>
                            </tr>
                        </table>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="copyright">

        <div>
            <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            <span id="busuanzi_container_site_pv">本站总访问量<span id="busuanzi_value_site_pv"></span>次</span>
        </div>
        <div>
            <span>©️ TIANJIN UNIVERSITY-DEEP REINFORCEMENT LEARNING LAB</span>
            <span style="margin-left: 25px;">ALL RIGHTS RESERVED</span><br/>
        </div>
    </div>
</section>
</body>
</html>
