<!doctype html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport"
          content="width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <link rel="icon" type="image/x-icon" href="/assets/favicon.ico">
    <title>Research</title>
    
    <link rel="stylesheet"
          href="https://fonts.loli.net/css?family=Manrope">
    <link rel="stylesheet"
          href="https://fonts.loli.net/css?family=Merriweather:900,900italic,300,300italic">
    <link href="https://fonts.loli.net/icon?family=Material+Icons" rel="stylesheet">
    <link rel="stylesheet" href="https://fonts.loli.net/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@20..48,100..700,0..1,-50..200" />
    <link href="https://fonts.loli.net/css?family=Lato:900,300" rel="stylesheet" type="text/css">
    <link rel="stylesheet"
          href="https://fonts.loli.net/css?family=Playfair+Display:400,900">

    <link rel="stylesheet" href="https://fonts.loli.net/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@48,400,0,0" />
    <style>
        body {
            cursor: default;
            padding: 0;
            margin: 0;
            background-color: #f0f0f0;
        }
    </style>
    <link rel="stylesheet" href="/assets/css/header.css">
    
        
            <link rel="stylesheet" href="/assets/css/research.css">
        
    
    <link rel="stylesheet" href="/assets/css/footer.css">
    

    

</head>
<body>
<script>
  // Wait for the DOM to fully load
  document.addEventListener("DOMContentLoaded", () => {
    // JavaScript code to select all <a> tags with class != "inner"
    const anchorTagsWithoutInnerClass = document.querySelectorAll('a:not(.inner)');

    // Loop through the selected anchor tags and add the target="_blank" attribute
    anchorTagsWithoutInnerClass.forEach(anchor => {
      anchor.setAttribute('target', '_blank');
    });
  });

</script>
<section id="header">
    <header>
        <div class="logo">
            <img src="/assets/image/logo.png" alt=""/>
        </div>
        <div class="action">
            <div class="tab">
                <a class="tab-item inner" href="/">HOME</a>
                <a class="tab-item inner" href="/news">NEWS</a>
                <a class="tab-item inner" href="/research">RESEARCH</a>
<!--                <a class="tab-item inner" href="/publications">PUBLICATIONS</a>-->
                 <div class="dropdown-menu">
                     <span class="text">PUBLICATIONS</span>
                     <div class="content">
                         <div onclick="window.location.href ='/conference'" class="tab-item item">
                             CONFERENCE
                         </div>
                         <div class="divider"></div>
                         <div onclick="window.location.href ='/journey'" class="tab-item item">
                             JOURNEY
                         </div>
                     </div>
                 </div>
                <!--<a class="tab-item inner" href="/software">SOFTWARE</a>-->
                <a class="tab-item inner" href="/team">TEAM</a>
<!--                <a class="tab-item inner" href="/ibbb">IBBB</a>-->
<!--                <a class="tab-item inner" href="/events">EVENTS</a>-->
                <a class="tab-item inner" href="/contact">CONTACT</a>
            </div>
            <div style="display: none" class="contact" onclick="window.location.href='/contact'">
                <span>Contact Us</span>
            </div>
            <div class="thin-menu">
                <input type="checkbox" id="menu-toggle">
                <label for="menu-toggle" class="menu-btn">
                    <i class="material-icons">menu</i>
                </label>
                <ul class="menu">
                    <li><a class="inner" href="/">HOME</a></li>
                    <li><a class="inner" href="/news">NEWS</a></li>
                    <li><a class="inner" href="/research">RESEARCH</a></li>
<!--                    <li><a class="inner" href="/publications">PUBLICATIONS</a></li>-->
                    <li><a class="inner" href="/conference">CONFERENCE</a></li>
                    <li><a class="inner" href="/journey">JOURNEY</a></li>
                    <!--<li><a class="inner" href="/software">SOFTWARE</a></li>-->
                    <li><a class="inner" href="/team">TEAM</a></li>
<!--                    <li><a class="inner" href="/ibbb">IBBB</a></li>-->
<!--                    <li><a class="inner" href="/events">EVENTS</a></li>-->
                    <li><a class="inner" href="/contact">CONTACT</a></li>
                </ul>
            </div>
        </div>
    </header>
</section>
<section id="research">
    <div class="research-title">
        Recent Research
    </div>

    <script>

      let go = (event) => {
        ;
        if (!event.target.classList.contains("banner")) {
          window.open(event.currentTarget.dataset.url)
        }
      }

      window.addEventListener("DOMContentLoaded", () => {
        document.querySelectorAll(".research-card").forEach(input => {
          input.addEventListener("click", go);
        });
      })

    </script>

    <div class="research-cards">
        
        
            <div class="research-card" data-url="/2023/09/11/Re2.html">
                <div class="research-image"
                     style="background-image: url('/assets/image/index/re2.png');
                     
                             background-size:contain;
                     ">
                </div>
                <div class="research-body">
                    <div class="research-card-title">
                        ERL-Re2: Efficient Evolutionary Reinforcement Learning with Shared State Representation and Individual Policy Representation
                    </div>
                    <div class="research-card-desc">
                        2023-09-11: Deep Reinforcement Learning (Deep RL) and Evolutionary Algorithms (EA) are two major paradigms of policy optimization with distinct learning principles. However, existing works on combining Deep RL and EA have two common drawbacks. In this paper, we propose Evolutionary Reinforcement Learning with Two-scale State Representation and Policy Representation.
                    </div>
                    <div class="research-card-action">
                        
                            <a target="_blank" href="https://arxiv.org/abs/2210.17375" class="banner">PDF</a>
                        
                            <a target="_blank" href="https://github.com/yeshenpy/ERL-Re2" class="banner">Code</a>
                        
                    </div>
                </div>
            </div>
        
            <div class="research-card" data-url="/2023/09/10/API.html">
                <div class="research-image"
                     style="background-image: url('/assets/image/research/iclr23_pi_pe.png');
                     
                             background-size:contain;
                     ">
                </div>
                <div class="research-body">
                    <div class="research-card-title">
                        Boosting Multiagent Reinforcement Learning via Permutation Invariant and Permutation Equivariant Networks
                    </div>
                    <div class="research-card-desc">
                        2023-09-10: The state space in Multiagent Reinforcement Learning (MARL) grows exponentially with the agent number. Such a curse of dimensionality results in poor scalability and low sample efficiency, inhibiting MARL for decades. To break this curse, we propose a unified agent permutation framework that exploits the permutation invariance (PI) and permutation equivariance (PE) inductive biases to reduce the multiagent state space. Our insight is that permuting the order of entities in the factored multiagent state space does not change the information.
                    </div>
                    <div class="research-card-action">
                        
                            <a target="_blank" href="https://openreview.net/pdf?id=OxNQXyZK-K8" class="banner">PDF</a>
                        
                            <a target="_blank" href="https://github.com/tjuHaoXiaotian/pymarl3" class="banner">Code</a>
                        
                    </div>
                </div>
            </div>
        
            <div class="research-card" data-url="/2023/08/02/MetaDiffuser.html">
                <div class="research-image"
                     style="background-image: url('/assets/image/research/metadiffuer.png');
                     
                             background-size:contain;
                     ">
                </div>
                <div class="research-body">
                    <div class="research-card-title">
                        MetaDiffuser: Diffusion Model as Conditional Planner for Offline Meta-RL
                    </div>
                    <div class="research-card-desc">
                        2023-08-02: Recently, diffusion model shines as a promising backbone for the sequence modeling paradigm in offline reinforcement learning. However, these works mostly lack the generalization ability across tasks with reward or dynamics change. To tackle this challenge, in this paper we propose a task-oriented conditioned diffusion planner for offline meta-RL(MetaDiffuser), which considers the generalization problem as conditional trajectory generation task with contextual representation. The key is to learn a context conditioned diffusion model which can generate task-oriented trajectories for planning across diverse tasks. To enhance the dynamics consistency of the generated trajectories while encouraging trajectories to achieve high returns, we further design a dual-guided module in the sampling process of the diffusion model. The proposed framework enjoys the robustness to the quality of collected warm-start data from the testing task and the flexibility to incorporate with different task representation method. The experiment results on MuJoCo benchmarks show that MetaDiffuser outperforms other strong offline meta-RL baselines, demonstrating the outstanding conditional generation ability of diffusion architecture.
                    </div>
                    <div class="research-card-action">
                        
                            <a target="_blank" href="http://proceedings.mlr.press/v202/ni23a/ni23a.pdf" class="banner">PDF</a>
                        
                            <a target="_blank" href="https://metadiffuser.github.io" class="banner">Site</a>
                        
                    </div>
                </div>
            </div>
        
    </div>
</section>

<section id="footer">
    <div class="footer-body">
        <div class="footer-body-holder">
            <div class="logos">
                <div class="logo"
                     style="background-image: url('/assets/image/logo-white.png');"></div>
                <div class="text">
                <span class="strong">
                Deep Reinforcement- Learning
                </span>
                    Laboratory
                </div>
            </div>
            <div class="footer-divider"></div>
            <div class="footer-actions">
                <div class="action-item extra-margin">
                    <div class="action-title">
                        Contact Us
                    </div>
                    <div class="action-body action-item">
                        <table style="border-spacing: 14px;border-collapse: collapse;user-select: text">
                            <tr>
                                <td style="width: 120px;vertical-align: top">
                                    <i class="material-icons">location_on</i>
                                </td>
                                <td style="width: 550px;vertical-align: top">
                                    天津市津南区海河教育园区雅观路135号天津大学软件学院55号楼A区
                                </td>
                            </tr>
                            <tr>
                                <td style="display: flex; align-items: center"><i class="material-icons">mail</i></td>
                                <td>
                                    <a class="hover-blue" href="mailto:jianye.hao@tju.edu.cn"
                                       style="color: white;text-decoration: underline;">
                                        jianye.hao@tju.edu.cn
                                    </a>
                                </td>
                            </tr>
                            <tr style="display: none">
                                <td>Phone:</td>
                                <td>(+22) 123 - 4567 - 900</td>
                            </tr>
                        </table>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="copyright">

        <div>
            <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            <span id="busuanzi_container_site_pv">本站总访问量<span id="busuanzi_value_site_pv"></span>次</span>
        </div>
        <div>
            <span>©️ TIANJIN UNIVERSITY-DEEP REINFORCEMENT LEARNING LAB</span>
            <span style="margin-left: 25px;">ALL RIGHTS RESERVED</span><br/>
        </div>
    </div>
</section>
</body>
</html>
