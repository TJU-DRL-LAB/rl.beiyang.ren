<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2024-12-09T14:27:13+08:00</updated><id>/feed.xml</id><title type="html">Your awesome title</title><subtitle>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.</subtitle><entry><title type="html">Multiagent Gumbel MuZero: Efficient Planning in Combinatorial Action Spaces</title><link href="/2024/05/05/MAMuZero.html" rel="alternate" type="text/html" title="Multiagent Gumbel MuZero: Efficient Planning in Combinatorial Action Spaces" /><published>2024-05-05T00:00:00+08:00</published><updated>2024-05-05T00:00:00+08:00</updated><id>/2024/05/05/MAMuZero</id><content type="html" xml:base="/2024/05/05/MAMuZero.html"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[To address the challenge of the exponential growth of the joint action space and the resulting exponential increase in search space, this paper extends the AlphaZero and MuZero algorithms to more complex multi-agent Markov decision processes, proposing two algorithms, Multiagent Gumbel AlphaZero and Multiagent Gumbel MuZero. These algorithms are based on resettable environment simulators and multi-agent world models constructed by neural networks, enabling efficient search and decision-making. An efficient without-replacement top-$k$ sampling algorithm is proposed for exponential action space, along with a high-quality policy improvement operator adapted to it, reducing the complexity and computational burden of Monte Carlo tree search while enhancing exploration capabilities. Besides, we propose a Centralized Planning with Decentralized Execution (CPDE) paradigm which uses centralized planning to accelerate policy learning while enabling decentralized execution for deployment. The algorithms achieve the best performance in testing environments such as StarCraft compared to model-free multi-agent reinforcement learning algorithms, with a 10-fold increase in learning sample efficiency while maintaining win rates.]]></summary></entry><entry><title type="html">Rethinking Decision Transformer via Hierarchical Reinforcement Learning</title><link href="/2024/05/05/ADT.html" rel="alternate" type="text/html" title="Rethinking Decision Transformer via Hierarchical Reinforcement Learning" /><published>2024-05-05T00:00:00+08:00</published><updated>2024-05-05T00:00:00+08:00</updated><id>/2024/05/05/ADT</id><content type="html" xml:base="/2024/05/05/ADT.html"><![CDATA[<h1 id="rethinking-decision-transformer-via-hierarchical-reinforcement-learning">Rethinking Decision Transformer via Hierarchical Reinforcement Learning</h1>

<h2 id="abstract">Abstract</h2>

<p>$Decision Transformer (DT) is an innovative algorithm leveraging recent advances of the transformer architecture in reinforcement learning (RL). However, a notable limitation of DT is its reliance on recalling trajectories from datasets, 
losing the capability to seamlessly Stitch sub-optimal trajectories together. In this work we introduce a general sequence modeling framework for studying sequential decision making through the lens of Hierarchical RL. At the time of making decisions, a high-level policy first proposes an ideal prompt for the current state, a low-level policy subsequently generates an action conditioned on the given prompt.  We show DT emerges as a special case of this framework with certain choices of high-level and low-level policies, and discuss the potential failure of these choices.  Inspired by these observations, we study how to jointly optimize the high-level and low-level policies to enable the stitching ability,  which further leads to the development of new offline RL algorithms.  Our empirical results clearly show that the proposed algorithms significantly surpass DT on several control and navigation benchmarks. We hope our contributions can inspire the integration of transformer architectures within the field of RL.</p>

<h2 id="autotuned-decision-transformer-adt">Autotuned Decision Transformer (ADT)</h2>

<h3 id="definition-of-adt">Definition of ADT</h3>

<p>Our algorithm is derived by considering a general framework that bridges transformer-based decision 
models with hierarchical reinforcement learning (HRL). In particular, we use the following hierarchical  representation of policy</p>

<p>$
\pi(a | s) = \int_{\mathcal{P}} \pi^{h}(p | s) \cdot  \pi^{l} (a | s, p) dp\, ,
$
where $\mathcal{P}$ is a set of prompts. To make a decision, the high-level policy $\pi^h$ first 
generates a prompt $p\in \mathcal{P}$, instructed by which the low-level policy $\pi^l$ returns an action conditioned on $p$.</p>

<p><img src="/assets/image/research/ADT/ADT.png" alt="Untitled" /></p>

<p>ADT jointly optimizes the hierarchical policies to overcomes the limitations of DT. An illustration 
of ADT architecture is provided in the figure above.  Similar to DT, ADT applies a transformer model 
for the low-level policy, it considers the following trajectory representation, 
$
\tau=\left(p_0, s_0, a_0, p_1, s_1, a_1, \ldots, p_T, s_T, a_T\right)
$
Here $p_i$ is the prompt generated by the high-level policy $p_i \sim \pi^h(\cdot | s_i)$, replacing 
the return-to-go prompt used by DT.  That is, for each trajectory in the offline dataset, we relabel
it by adding a prompt generated by the high-level policies for each transition. 
Armed with this general hierarchical decision framework, we propose two algorithms that apply different 
high-level prompting generation strategy while sharing a unified low-level policy optimization framework. 
We learn a high-level policy $\pi_\omega\approx \pi^h$ with parameters $\phi$, and a low-level polic y $\pi_\theta \approx \pi^l$ with parameters $\theta$.</p>

<h3 id="value-prompted-autotuned-decision-transformer-v-adt">Value-prompted Autotuned Decision Transformer (V-ADT)</h3>

<p>Our first algorithm, \emph{Value-promped Autotuned Decision Transformer (V-ADT)}, uses scalar values as prompts. But unlike DT, it applies a more principled design of value prompts instead of return-to-go.<br />
V-ADT aims to answer two questions: what is the maximum achievable value starting from a state $s$, and what action should be taken to achieve such a value?<br />
The optimal value of this empirical MDP presented by the offline dataset is
$
V_{\mathcal{D}}^*(s)=\max _{a: \pi_{\mathcal{D}}(a \mid s)&gt;0} r(s, a)+\gamma \mathbb{E}_{s^{\prime} \sim P_{\mathcal{D}}(\cdot \mid s, a)}.
$</p>

<p>Let $Q_{\mathcal{D}}^*(s, a)$ be the corresponding state-action value. $V_{\mathcal{D}}^*$ 
is known as the in-sample optimal value in offline RL. 
We now describe how V-ADT jointly optimizes high and low level policies to facilitate stitching.</p>

<h4 id="high-level-policy">High-Level policy</h4>

<p>High-Level policy V-ADT adopts a deterministic policy $\pi_\omega: \mathcal{S} \rightarrow \mathbb{R}$, which predicts the in-sample optimal value $\pi
_\omega \approx V_{\mathcal{D}}^*$. Since we already have an approximated in-sample optimal value $V_\phi$,
we let $\pi_\omega=V_\phi$. This high-level policy offers two key advantages. 
First, this approach efficiently facilitates information backpropagation towards earlier states on a trajectory, 
addressing a major limitation of DT. This is achieved by using $V_{\mathcal{D}}^*$ as the value prompt,
ensuring that we have precise knowledge of the maximum achievable return for any state.
Making predictions conditioned on $R^*(s)$ is not enough for policy optimization, 
since $R^*(s)=\max _{\tau \in \mathcal{T}(s)} R(\tau)$ only gives a lower bound on $V_{\mathcal{D}}^*(s)$ 
and thus would be a weaker guidance (see Section 3.1 for detailed discussions). 
Second, the definition of $V_{\mathcal{D}}^*$ exclusively focuses on the optimal value derived 
from observed data and thus avoids out-of-distribution returns. This prevents the low-level policy 
from making decisions conditioned on prompts that require extrapolation.</p>

<h4 id="low-level-policy">Low-Level policy</h4>

<p>Low-Level policy Directly training the model to predict the trajectory, as done in DT, is not suitable for our approach. This is because the action $a_t$ observed in the data may not necessarily correspond to the action at state $s_t$ that leads to the return $V_{\mathcal{D}}^*\left(s_t\right)$. However, the probability of selecting $a_t$ should be proportional to the value of this action. Thus, we use advantage-weighted regression to learn the low-level policy: given trajectory data, the objective is defined as
$
\mathcal{L}(\theta)=-\sum_{t=0}^T \exp \left(\frac{Q_\psi\left(s_t, a_t\right)-V_\phi\left(s_t\right)}{\alpha}\right) \log \pi_\theta\left(a_t \mid s_t, \pi_\omega\left(s_t\right)\right),
$
where $\alpha&gt;0$ is a hyper-parameter. The low-level policy takes the output of high-level policy as input. This guarantees no discrepancy between train and test value prompt used by the policies. We note that the only difference of this compared to the standard maximum log-likelihood objective for sequence modeling is to apply a weighting for each transition. One can easily implement this with trajectory data for a transformer. In practice we also observe that the tokenization scheme when processing the trajectory data affects the performance of ADT. Instead of treating the prompt $p_t$ as a single token as in DT, we find it is beneficial to concatenate $p_t$ and $s_t$ together and tokenize the concatenated vector.</p>

<h3 id="goal-prompted-autotuned-decision-transformer-g-adt">Goal-prompted Autotuned Decision Transformer (G-ADT)</h3>

<p>In HRL, the high-level policy often considers a latent action space. Typical choices of latent actions includes sub-goal, skills, and options. We consider goal-reaching problem as an example and use subgoals as latent actions, which leads to our second algorithm, Goal-promped Autotuned Decision Transformer (G-ADT). Let $\mathcal{G}$ be the goal space ${ }^3$. The goal-conditioned reward function $r(s, a, g)$ provides the reward of taking action $a$ at state $s$ for reaching the goal $g \in \mathcal{G}$. Let $V(s, g)$ be the universal value function defined by the goal-conditioned rewards. Similarly, we define $V_{\mathcal{D}}^*(s, g)$ and $Q_{\mathcal{D}}^*(s, a, g)$ the in-sample optimal universal value function. We also train $V_\phi \approx V_{\mathcal{D}}^*$ and $Q_\psi \approx Q_{\mathcal{D}}^*$ to approximate the universal value functions. We now describe how G-ADT jointly optimizes the policies.</p>

<h4 id="high-level-policy-1">High-Level policy</h4>

<p>G-ADT considers $\mathcal{P}=\mathcal{G}$ and uses a high-level policy $\pi_\omega: \mathcal{S} \rightarrow \mathcal{G}$. To find a shorter path, the high-level policy $\pi_\omega$ generates a sequence of sub-goals $g_t=\pi_\omega\left(s_t\right)$ that guides the learner step-by-step towards the final goal. We use a sub-goal that lies in $k$-steps further from the current state, where $k$ is a hyper-parameter of the algorithm tuned for each domain (Badrinath et al., 2023; Park et al., 2023). In particular, given trajectory data (4), the high-level policy learns the optimal $k$-steps jump using the recently proposed Hierarchical Implicit Q-learning (HIQL) algorithms:
$
\begin{aligned}
&amp; \mathcal{L}(\phi)=-\sum_{t=0}^T \exp \left(\frac{\mathcal{A}_{\text {high }}}{\alpha}\right) \log \pi_\omega\left(s_{t+k} \mid s_t, g\right) <br />
&amp; \mathcal{A}_{\text {high }}=\sum_{t^{\prime}=t}^{k-1} \gamma^{t^{\prime}-t} r\left(s_{t^{\prime}}, a_{t^{\prime}}, g\right)+\gamma^k V_\phi\left(s_{t+k}, g\right)-V_\phi\left(s_t, g\right)
\end{aligned}
$</p>

<h4 id="low-level-policy-1">Low-Level policy</h4>

<p>The low-level policy in G-ADT learns to reach the sub-goal generated by the high-level policy. GADT shares the same low-level policy objective as V-ADT. Given trajectory data, it considers the following
$
\mathcal{L}(\theta)=-\sum_{t=0}^T \exp \left(\frac{\mathcal{A}_{\text {low }}}{\alpha}\right) \cdot \log \pi_\theta\left(a_t \mid s_t, \pi_\omega\left(s_t\right)\right),
$
where $\mathcal{A}_{\text {low }}=Q_\psi\left(s_t, a_t, \pi_\omega\left(s_t\right)\right)-V_\phi\left(s_t, \pi_\omega\left(s_t\right)\right)$. Note that this is exactly the same as that in V-ADT except that the advantages $\mathcal{A_\text{low}}$ are computed by  universal value functions. G-ADT also applies the same tokenization method as V-ADT by first concatenating $\pi_\omega(s_t)$ and $s_t$ together. This concludes the description of the G-ADT algorithm.</p>

<h2 id="experiments">Experiments</h2>

<p>Table 1 and 2 present the performance of two variations of ADT evaluated on offline datasets. ADT significantly outperforms prior transformer-based decision making algorithms. Compared to DT and QLDT, two transformer-based algorithms for decision making, V-ADT  exhibits significant superiority especially on AntMaze and Kitchen which require the stitching ability to success. 
Meanwhile,   Table 2 shows that G-ADT significantly outperforms WT, an algorithm that uses sub-goal as prompt for a transformer policy.  We note that ADT enjoys comparable performance with state-of-the-art offline RL methods. 
For example, V-ADT outperforms all offline RL baselines in Mujoco problems.  In AntMaze and Kitchen, V-ADT matches the performance of IQL, and significantly outperforms TD3+BC and CQL. 
Table 2 concludes with similar findings for G-ADT.</p>

<p><img src="/assets/image/research/ADT/exp1.png" alt="Untitled" /></p>

<p><img src="/assets/image/research/ADT/exp2.png" alt="Untitled" /></p>]]></content><author><name></name></author><summary type="html"><![CDATA[In this work we introduce a general sequence modeling framework for studying sequential decision making through the lens of Hierarchical RL. At the time of making decisions, a high-level policy first proposes an ideal prompt for the current state, a low-level policy subsequently generates an action conditioned on the given prompt. We show DT emerges as a special case of this framework with certain choices of high-level and low-level policies, and discuss the potential failure of these choices. Inspired by these observations, we study how to jointly optimize the high-level and low-level policies to enable the stitching ability, which further leads to the development of new offline RL algorithms. .]]></summary></entry><entry><title type="html">What About Inputting Policy in Value Function: Policy Representation and Policy-extended Value Function Approximator</title><link href="/2023/10/21/PeVFA.html" rel="alternate" type="text/html" title="What About Inputting Policy in Value Function: Policy Representation and Policy-extended Value Function Approximator" /><published>2023-10-21T00:00:00+08:00</published><updated>2023-10-21T00:00:00+08:00</updated><id>/2023/10/21/PeVFA</id><content type="html" xml:base="/2023/10/21/PeVFA.html"><![CDATA[<h1 id="what-about-inputting-policy-in-value-function-policy-representation-and-policy-extended-value-function-approximator">What About Inputting Policy in Value Function: Policy Representation and Policy-extended Value Function Approximator</h1>

<h1 id="abstract">Abstract</h1>

<p>One fundamental element of RL is value function which defines the long-term evaluation of a policy. With function approximation (e.g., deep neural networks), a value function approximator (VFA) is able to approximate the values of a policy under large and continuous state spaces.</p>

<p>We study Policy-extended Value Function Approximator (PeVFA) in Reinforcement Learning (RL), which extends conventional value function approximator (VFA) to take as input not only the state (and action) but also an explicit policy representation. Such an extension enables PeVFA to preserve values of multiple policies at the same time and brings an appealing characteristic, i.e., <em>value generalization among policies</em>.</p>

<p>We formally analyze the value generalization under Generalized Policy Iteration (GPI). From theoretical and empirical lens, we show that generalized value estimates offered by PeVFA
may have lower initial approximation error to true values of successive policies, which is expected to improve consecutive value approximation during GPI. Based on above clues, we introduce a new form of GPI with PeVFA which leverages the value generalization along policy improvement path. Moreover, we propose a representation learning framework for RL policy, providing several approaches to learn effective policy embeddings from policy network parameters or stateaction pairs. In our experiments, we evaluate the efficacy of value generalization offered by PeVFA and policy representation learning in several OpenAI Gym continuous control tasks.</p>

<h1 id="policy-extended-value-function-approximation-pevfa">Policy-extended Value Function Approximation (PeVFA)</h1>

<h3 id="definition-of-pevfa">Definition of PeVFA</h3>

<p>Since the <em>conventional value functions</em> are defined on a single policy, how can a value function estimates the values of <em>multiple policies</em>?</p>

<p>To answer this question, we propose <strong>Policy-extended Value Function Approximator</strong> (<strong>PeVFA</strong>):</p>

\[\mathbb{V}(s,\chi_{\pi}) = \mathbb{E}_{\pi} = \left[ \sum_{t=0}^{\infty} \gamma^{t}r_{t+1} | s_0=s \right], \text{for all }s \in S, \pi \in \Pi.\]

<ul>
  <li>Additionally takes a <em>policy representation</em> <strong>$\chi_{\pi}$</strong> as input (just consider we have one here, and we will discuss how to get $<strong>\chi_{\pi}</strong>$ later)</li>
  <li>Thus, has the ability/capacity to preserve values of multiple policies</li>
  <li>Values are expected to <em>generalize among policies</em> <em>**</em>with FA</li>
</ul>

<h3 id="two-kinds-of-value-generalization-of-pevfa">Two Kinds of Value Generalization of PeVFA</h3>

<p>It is exciting to see the value generalization among policies brought by PeVFA. We consider two kinds of value generalization below, which are also illustrated in the figure below:</p>

<ul>
  <li><em>Global</em>: from learned policies ($\pi \in \Pi_0$) to unlearned ones ($\pi^{\prime} \in \Pi_1$)</li>
  <li><em>Local</em>: from previous policies (${\pi_i}<em>{i=0}^t$) to the successive one ($\pi</em>{t+1}$) along the <em>policy improvement path of GPI (our focus of this paper)</em></li>
</ul>

<p><img src="/assets/image/research/PeVFA/Untitled 1.png" alt="Untitled" /></p>

<h3 id="general-policy-iteration-with-pevfa">General Policy Iteration with PeVFA</h3>

<p>Naturally, the following questions are:</p>

<ul>
  <li><em>Is value generalization necessarily beneficial?</em></li>
  <li><em>If so, how can we utilize it to improve RL?</em></li>
</ul>

<p>For the first question, we formularize the value generalization of PeVFA and analyze the situations where the value generalization can be beneficial to typical RL process. We refer interested readers to our paper for more details.</p>

<p>For the second question, we expect to leverage the value generalization of PeVFA to facilitate RL in a general way. To be concrete, we propose General Policy Iteration with PeVFA (GPI with PeVFA). A general description of RL algorithm under the paradigm of GPI with PeVFA is shown below:</p>

<p><img src="/assets/image/research/PeVFA/Untitled 2.png" alt="Untitled" /></p>

<p>For each iteration, the interaction experiences of current policy and the policy representation are stored in a buffer (line 3-4). At an interval of $M$ iterations, PeVFA is trained via value approximation for previous policies with the stored data and the policy representation model is updated according to the method used (line 5-8). This part is unique to PeVFA for preservation and generalization of knowledge of historical policies. Next, value approximation for current policy is performed with PeVFA (line 9). A key difference here is that the generalized value estimates (i.e., $\mathbb{V}<em>{t-1}(\chi</em>{\pi_t})$) are used as start points. Afterwards, a successive policy is obtained from typical policy improvement (line 10).</p>

<p>Algorithm 1 can be implemented in different ways and we propose an instance implemented based on PPO (Schulman et al. 2017) in our experiments.</p>

<h1 id="a-policy-representation-learning-framework">A Policy Representation Learning Framework</h1>

<p>To derive practical deep RL algorithms, one key point is policy representation, i.e., a low-dimensional embedding of RL policy. Now we come back to the problem of obtaining the policy representation $\chi_{\pi}$ for a typical Deep RL policy $\pi$.</p>

<p>The framework of policy representation learning proposed in our work is shown below. Policy network parameters used for <em>Origin Policy Representation (OPR)</em> or policy state-action pairs used for <em>Surface Policy Representation (SPR)</em> are fed into policy encoder with permutation-invariant (PI) transformations followed by an MLP, producing the representation $\chi_\pi$.</p>

<p>Afterwards, $\chi_{\pi}$ can be trained by gradients from the value approximation loss of PeVFA (i.e., End-to-End), as well as the auxiliary loss of policy recovery or contrastive learning (i.e., InfoNCE) loss.</p>

<p><img src="/assets/image/research/PeVFA/Untitled 3.png" alt="Untitled" /></p>

<h1 id="experiments">Experiments</h1>

<p>According to the paradigm of GPI with PeVFA and the general algorithm shown above, we replace the value network of vanilla PPO with PeVFA and obtain PPO-PeVFA. Experiments are conducted in OpenAI Gym continuous control tasks.</p>

<p>The aggregated performance of considered algorithms for comparative evaluation is shown below.</p>

<p><img src="/assets/image/research/PeVFA/Untitled 4.png" alt="Untitled" /></p>

<h3 id="visual-analysis">Visual Analysis</h3>

<p>The 2D t-SNE visualization of learned policy representation is shown in the figure below. We can observe that policies from different trials are locally continuous and show different modes of embedding trajectories due to random initialization and optimization; while a global evolvement among trials emerges with respect to policy performance.</p>

<p><img src="/assets/image/research/PeVFA/Untitled 5.png" alt="Untitled" /></p>]]></content><author><name></name></author><summary type="html"><![CDATA[We study Policy-extended Value Function Approximator (PeVFA) in Reinforcement Learning (RL), which extends conventional value function approximator (VFA) to take as input not only the state (and action) but also an explicit policy representation. Such an extension enables PeVFA to preserve values of multiple policies at the same time and brings an appealing characteristic, i.e., *value generalization among policies*.]]></summary></entry><entry><title type="html">ERL-Re2: Efficient Evolutionary Reinforcement Learning with Shared State Representation and Individual Policy Representation</title><link href="/2023/09/10/Re2.html" rel="alternate" type="text/html" title="ERL-Re2: Efficient Evolutionary Reinforcement Learning with Shared State Representation and Individual Policy Representation" /><published>2023-09-10T00:00:00+08:00</published><updated>2023-09-10T00:00:00+08:00</updated><id>/2023/09/10/Re2</id><content type="html" xml:base="/2023/09/10/Re2.html"><![CDATA[<p>本次介绍的是新的进化强化学习范式<strong>ERL-Re$^2$</strong>。该范式充分融合了进化算法与强化学习用于策略优化，并实现了显著的性能增益与效果。</p>

<p>进化算法与与强化学习是两类不同的优化方式，擅长解决不同的优化问题，并且都拥有很大，很活跃的社区，本次介绍的ICLR2023的工作就是为了将两个社区连接起来，充分利用两种不同优化算法各自的优势来实现策略搜索与性能提升 。目前代码已经开源。</p>

<h2 id="background">Background</h2>

<p><strong>强化学习 Reinforcement Learning</strong>（RL）可以通过环境试错和梯度更新来高效地学习。然而，众所周知，RL鲁棒性差，探索性差，并且在梯度信号有噪声和信息量较少（sparse）的情况下，难以高效训练。
<strong>进化算法 Evolutionary Algorithms</strong>（EA）是一类黑箱优化方法，主要是维护一个个体的种群，而不是像RL只维护一个个体，通过随机扰动的方式来提升个体获得可行解。与RL不同的是，传统EA是无梯度优化方法，并具有几个优点：
（1）强大的探索能力、（2）鲁棒性和稳定的收敛、（3）采用累计奖励评价个体，不关心单步奖励，因此对奖励信号不敏感。
尽管有这些优点，EA的一个主要瓶颈是群体的迭代评估而导致的低样本效率。具体来说，EA需要种群中的每个个体与环境真实交互来获得适应度（性能表现），最终根据种群中不同个体的适应度来进行种群提升。</p>

<p>很多工作都在研究如何将EA和RL的融合起来，取长补短，优势互补。其中最具有代表性的当属2018年提出的演化强化学习框架（ERL），将Genetic Algorithm（GA）与DDPG进行了融合。除了维护强化学习的actor和critic，ERL额外维护一个的actor的种群。为了融合双方的优点，EA与环境交互产生的多样性的样本会提供给RL用于off policy优化，这一方面解决了EA样本利用率低的问题，另一方面缓解了RL探索弱无法寻找到多样数据的问题。除此之外，优化后的RL 策略会定期注入到种群中参与种群进化，如果RL策略优于种群策略，那么RL策略则会促进种群的进化，否则则会被淘汰掉，不影响种群进化。最终EA与RL优势互补，在MuJoCo上实现了对DDPG算法的显著提升。（这里的EA演化都是直接在策略的参数上进行扰动优化，例如k点交叉是交换两个网络中某些层的参数，变异则直接将高斯扰动添加到网络参数上）</p>

<h2 id="motivation">Motivation</h2>

<p>ERL工作后，许多基于ERL基本框架的相关工作随之产生，例如CERL，PDERL, CEM-RL等。由于都遵循基本的ERL框架，导致这些算法都面临着两个基本问题：</p>

<ul>
  <li>所有的策略都单独学习自己的状态表征，维护各自的网络，忽略了有效知识的共享。</li>
  <li>对于演化算子，参数层面的策略优化不能保证个体的行为语义，容易造成策略灾难性崩溃。</li>
</ul>

<h2 id="the-concept-of-two-scale-state-representation-and-policy-representation">The Concept of Two-Scale State Representation and Policy Representation</h2>
<p>为了解决上述问题，我们提出了<strong>基于双尺度表征的策略构建</strong>（<strong>Two-scale representation-based policy construction</strong>）。在此基础上，我们维护和优化EA群体和RL的策略。具体来说。EA和RL Agent的策略都是由一个共享的非线性状态表征$Z{\phi}$和一个独立的线性策略表征 $W$ 组成。Agent $i$ 通过结合共享状态表征和策略表征做出决策：</p>

<p><img src="/assets/image/research/Re2/1.png" alt="1" /></p>

<p>直观地，我们希望共享状态表征$Z{\phi}$对学习过程中遇到的所有可能的策略都有用。它应该包含环境中与决策有关的一般特征，例如，共同的知识，而不是针对某一个策略。由于共享状态表示$Z_{\phi}$，Agent不需要独立地表征状态。因此，更高的效率和更具表现力的状态表征可以通过EA群体和RL Agent共同得到。由于$Z{\phi}$的高表达性，每个独立的策略表征可以由一个简单的线性形式构成，这更易于优化与探索。</p>

<p><img src="/assets/image/research/Re2/2.png" alt="6a224755e7478c57e45d7a91c514a23c.png" /></p>

<p>上图是ERL（及后续工作）与我们提出的双尺度表征框架ERL-Re²的对比图。其中左图中的策略主要由传统的非线性神经网络构成。右图中的圆形表示线性策略表征，六边形则表示非线性共享状态表征，用于知识共享。*</p>

<p><em><img src="/assets/image/research/Re2/3.png" alt="3" /></em></p>

<p><strong>算法优化流程</strong>:整体优化流程如上图所示，具体来说，算法每次在由共享状态表征$Z{\phi}$构建的线性策略空间 $\Pi{\phi}$ 中进行策略搜索，对线性策略进行优化。优化后我们对共享状态表征进行优化，优化的方向为对于所有个体（包括EA和RL)都有益的方向，从而达到有效的知识共享，构建对于所有个体都有利的策略空间。如此循环迭代实现知识的高效传递与策略的快速优化。下面我们介绍如何进行共享表征的优化，以及如何在线性空间如何更加高效地演化。</p>

<h2 id="optimizing-the-shared-state-representation-for-a-superior-policy-space"><em>Optimizing the Shared State Representation for A Superior Policy Space</em></h2>
<p>为了构建所有个体都有益的状态表征从而实现高效地知识共享，我们提出基于所有EA和RL策略的价值函数最大化来学习共享状态表征。对于EA策略，我们根据EA群体ℙ中的线性策略表示 ，学习策略拓展值函数（PeVFA, 通过将策略表征作为输入，实现一个价值函数估计多个不同策略value的目的）。对于RL策略，我们使用原始RL算法的值函数提供更新方向。两个值函数都是通过TD error进行优化的，损失如下：</p>

<p><img src="/assets/image/research/Re2/4.png" alt="4" /></p>

<p>EA中的个体和RL个体都能分别从PeVFA和RL critic获得各自的优化方向. 而我们想构建的共享状态表征应该有助于所有个体的探索与优化，因此共享表征的更新方向应该考虑到EA和RL，因此我们定义了如下损失：</p>

<p><img src="/assets/image/research/Re2/5.png" alt="5" /><br />
通过优化上述损失，共享状态表征能够向着一个统一的优化方向进行优化，从而构建一个有助于所有个体的线性策略空间，使得EA和RL能够更加高效地探索与提升。</p>

<h2 id="optimizing-the-policy-representation-by-evolution-and-reinforcement">Optimizing the Policy Representation by Evolution and Reinforcement</h2>
<p>对于种群的进化，我们首先需要得到适配度（fitness），所产生的样本开销是EA的一个主要瓶颈，特别是当种群很大时。为此，我们提出了一个基于PeVFA 的新的适应度函数。对于每个Agent ，我们让Agent与环境交互$𝐻$步，随后使用PeVFA进行估值来节省样本开销。 fitness被定义如下：</p>

<p><img src="/assets/image/research/Re2/6.png" alt="6" /></p>

<p>对于遗传进化的过程，传统的交叉变异都是直接在整个策略的参数空间进行扰动，由于策略往往是由非线性的神经网络构建的。单独的改变神经网络的某些参数可能会造成策略行为的坍塌与崩溃。为了解决这个问题，我们提出了新的behavior-level 交叉和变异，允许在指定的行动维度上施加扰动，同时对其他动作不产生任何干扰。具体来说，由于共享状态表征的构建，演化发生在线性策略表征空间，线性策略表征的每个维度对应决策的一个动作，因此我们可以直接交换表征的某一维度的参数，而实现两个策略的某个动作的交叉，而不对其他动作产生扰动（behavior-level crossover），同样扰动也可以被单独加在表征的某个特定维度上，不对其他动作产生扰动（behavior-level mutation）。behavior-level 交叉变异的示意图如下图所示。</p>

<p><img src="/assets/image/research/Re2/7.png" alt="7" /></p>

<p>最后整个实验的伪代码：</p>

<p><img src="/assets/image/research/Re2/8.png" alt="8" /></p>

<h2 id="experiments">Experiments</h2>
<p>本文实验主要在MUJOCO的6个常用的task上验证了方法的有效性，基本上都有大幅度的性能增益，达到了在这个benchmark上的ERL方向的新SOTA。除此之外，本文也尝试了一些其他环境和算法，大家可以到原文中查看更多的细节。</p>

<p><em><img src="/assets/image/research/Re2/10.png" alt="10" /><img src="/assets/image/research/Re2/9.png" alt="9" /></em></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Deep Reinforcement Learning (Deep RL) and Evolutionary Algorithms (EA) are two major paradigms of policy optimization with distinct learning principles, i.e., gradient-based v.s. gradient-free. An appealing research direction is integrating Deep RL and EA to devise new methods by fusing their complementary advantages. However, existing works on combining Deep RL and EA have two common drawbacks: 1) the RL agent and EA agents learn their policies individually, neglecting efficient sharing of useful common knowledge; 2) parameter-level policy optimization guarantees no semantic level of behavior evolution for the EA side. In this paper, we propose Evolutionary Reinforcement Learning with Two-scale State Representation and Policy Representation (ERL-Re^2), a novel solution to the aforementioned two drawbacks. The key idea of ERL-Re^2 is two-scale representation: all EA and RL policies share the same nonlinear state representation while maintaining individual} linear policy representations. The state representation conveys expressive common features of the environment learned by all the agents collectively; the linear policy representation provides a favorable space for efficient policy optimization, where novel behavior-level crossover and mutation operations can be performed. Moreover, the linear policy representation allows convenient generalization of policy fitness with the help of the Policy-extended Value Function Approximator (PeVFA), further improving the sample efficiency of fitness estimation. The experiments on a range of continuous control tasks show that ERL-Re^2 consistently outperforms advanced baselines and achieves the State Of The Art (SOTA).]]></summary></entry><entry><title type="html">Boosting Multiagent Reinforcement Learning via Permutation Invariant and Permutation Equivariant Networks</title><link href="/2023/09/09/API.html" rel="alternate" type="text/html" title="Boosting Multiagent Reinforcement Learning via Permutation Invariant and Permutation Equivariant Networks" /><published>2023-09-09T00:00:00+08:00</published><updated>2023-09-09T00:00:00+08:00</updated><id>/2023/09/09/API</id><content type="html" xml:base="/2023/09/09/API.html"><![CDATA[<script>
    window.addEventListener('load', function() {
        window.location.href = 'https://github.com/tjuHaoXiaotian/pymarl3';
    });
</script>]]></content><author><name></name></author><summary type="html"><![CDATA[The state space in Multiagent Reinforcement Learning (MARL) grows exponentially with the agent number. Such a curse of dimensionality results in poor scalability and low sample efficiency, inhibiting MARL for decades. To break this curse, we propose a unified agent permutation framework that exploits the permutation invariance (PI) and permutation equivariance (PE) inductive biases to reduce the multiagent state space. Our insight is that permuting the order of entities in the factored multiagent state space does not change the information.]]></summary></entry><entry><title type="html">MetaDiffuser: Diffusion Model as Conditional Planner for Offline Meta-RL</title><link href="/2023/08/01/MetaDiffuser.html" rel="alternate" type="text/html" title="MetaDiffuser: Diffusion Model as Conditional Planner for Offline Meta-RL" /><published>2023-08-01T00:00:00+08:00</published><updated>2023-08-01T00:00:00+08:00</updated><id>/2023/08/01/MetaDiffuser</id><content type="html" xml:base="/2023/08/01/MetaDiffuser.html"><![CDATA[<script>
    window.addEventListener('load', function() {
        window.location.href = 'https://metadiffuser.github.io/';
    });
</script>]]></content><author><name></name></author><summary type="html"><![CDATA[Recently, diffusion model shines as a promising backbone for the sequence modeling paradigm in offline reinforcement learning. However, these works mostly lack the generalization ability across tasks with reward or dynamics change. To tackle this challenge, in this paper we propose a task-oriented conditioned diffusion planner for offline meta-RL(MetaDiffuser), which considers the generalization problem as conditional trajectory generation task with contextual representation. The key is to learn a context conditioned diffusion model which can generate task-oriented trajectories for planning across diverse tasks. To enhance the dynamics consistency of the generated trajectories while encouraging trajectories to achieve high returns, we further design a dual-guided module in the sampling process of the diffusion model. The proposed framework enjoys the robustness to the quality of collected warm-start data from the testing task and the flexibility to incorporate with different task representation method. The experiment results on MuJoCo benchmarks show that MetaDiffuser outperforms other strong offline meta-RL baselines, demonstrating the outstanding conditional generation ability of diffusion architecture.]]></summary></entry></feed>