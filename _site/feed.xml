<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2023-12-14T15:56:28+08:00</updated><id>/feed.xml</id><title type="html">Your awesome title</title><subtitle>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.</subtitle><entry><title type="html">What About Inputting Policy in Value Function: Policy Representation and Policy-extended Value Function Approximator</title><link href="/2023/10/21/PeVFA.html" rel="alternate" type="text/html" title="What About Inputting Policy in Value Function: Policy Representation and Policy-extended Value Function Approximator" /><published>2023-10-21T00:00:00+08:00</published><updated>2023-10-21T00:00:00+08:00</updated><id>/2023/10/21/PeVFA</id><content type="html" xml:base="/2023/10/21/PeVFA.html"><![CDATA[<h1 id="what-about-inputting-policy-in-value-function-policy-representation-and-policy-extended-value-function-approximator">What About Inputting Policy in Value Function: Policy Representation and Policy-extended Value Function Approximator</h1>

<h1 id="abstract">Abstract</h1>

<p>One fundamental element of RL is value function which defines the long-term evaluation of a policy. With function approximation (e.g., deep neural networks), a value function approximator (VFA) is able to approximate the values of a policy under large and continuous state spaces.</p>

<p>We study Policy-extended Value Function Approximator (PeVFA) in Reinforcement Learning (RL), which extends conventional value function approximator (VFA) to take as input not only the state (and action) but also an explicit policy representation. Such an extension enables PeVFA to preserve values of multiple policies at the same time and brings an appealing characteristic, i.e., <em>value generalization among policies</em>.</p>

<p>We formally analyze the value generalization under Generalized Policy Iteration (GPI). From theoretical and empirical lens, we show that generalized value estimates offered by PeVFA
may have lower initial approximation error to true values of successive policies, which is expected to improve consecutive value approximation during GPI. Based on above clues, we introduce a new form of GPI with PeVFA which leverages the value generalization along policy improvement path. Moreover, we propose a representation learning framework for RL policy, providing several approaches to learn effective policy embeddings from policy network parameters or stateaction pairs. In our experiments, we evaluate the efficacy of value generalization offered by PeVFA and policy representation learning in several OpenAI Gym continuous control tasks.</p>

<h1 id="policy-extended-value-function-approximation-pevfa">Policy-extended Value Function Approximation (PeVFA)</h1>

<h3 id="definition-of-pevfa">Definition of PeVFA</h3>

<p>Since the <em>conventional value functions</em> are defined on a single policy, how can a value function estimates the values of <em>multiple policies</em>?</p>

<p>To answer this question, we propose <strong>Policy-extended Value Function Approximator</strong> (<strong>PeVFA</strong>):</p>

\[\mathbb{V}(s,\chi_{\pi}) = \mathbb{E}_{\pi} = \left[ \sum_{t=0}^{\infty} \gamma^{t}r_{t+1} | s_0=s \right], \text{for all }s \in S, \pi \in \Pi.\]

<ul>
  <li>Additionally takes a <em>policy representation</em> <strong>$\chi_{\pi}$</strong> as input (just consider we have one here, and we will discuss how to get $<strong>\chi_{\pi}</strong>$ later)</li>
  <li>Thus, has the ability/capacity to preserve values of multiple policies</li>
  <li>Values are expected to <em>generalize among policies</em> <em>**</em>with FA</li>
</ul>

<h3 id="two-kinds-of-value-generalization-of-pevfa">Two Kinds of Value Generalization of PeVFA</h3>

<p>It is exciting to see the value generalization among policies brought by PeVFA. We consider two kinds of value generalization below, which are also illustrated in the figure below:</p>

<ul>
  <li><em>Global</em>: from learned policies ($\pi \in \Pi_0$) to unlearned ones ($\pi^{\prime} \in \Pi_1$)</li>
  <li><em>Local</em>: from previous policies (${\pi_i}<em>{i=0}^t$) to the successive one ($\pi</em>{t+1}$) along the <em>policy improvement path of GPI (our focus of this paper)</em></li>
</ul>

<p><img src="/assets/image/research/PeVFA/Untitled 1.png" alt="Untitled" /></p>

<h3 id="general-policy-iteration-with-pevfa">General Policy Iteration with PeVFA</h3>

<p>Naturally, the following questions are:</p>

<ul>
  <li><em>Is value generalization necessarily beneficial?</em></li>
  <li><em>If so, how can we utilize it to improve RL?</em></li>
</ul>

<p>For the first question, we formularize the value generalization of PeVFA and analyze the situations where the value generalization can be beneficial to typical RL process. We refer interested readers to our paper for more details.</p>

<p>For the second question, we expect to leverage the value generalization of PeVFA to facilitate RL in a general way. To be concrete, we propose General Policy Iteration with PeVFA (GPI with PeVFA). A general description of RL algorithm under the paradigm of GPI with PeVFA is shown below:</p>

<p><img src="/assets/image/research/PeVFA/Untitled 2.png" alt="Untitled" /></p>

<p>For each iteration, the interaction experiences of current policy and the policy representation are stored in a buffer (line 3-4). At an interval of $M$ iterations, PeVFA is trained via value approximation for previous policies with the stored data and the policy representation model is updated according to the method used (line 5-8). This part is unique to PeVFA for preservation and generalization of knowledge of historical policies. Next, value approximation for current policy is performed with PeVFA (line 9). A key difference here is that the generalized value estimates (i.e., $\mathbb{V}<em>{t-1}(\chi</em>{\pi_t})$) are used as start points. Afterwards, a successive policy is obtained from typical policy improvement (line 10).</p>

<p>Algorithm 1 can be implemented in different ways and we propose an instance implemented based on PPO (Schulman et al. 2017) in our experiments.</p>

<h1 id="a-policy-representation-learning-framework">A Policy Representation Learning Framework</h1>

<p>To derive practical deep RL algorithms, one key point is policy representation, i.e., a low-dimensional embedding of RL policy. Now we come back to the problem of obtaining the policy representation $\chi_{\pi}$ for a typical Deep RL policy $\pi$.</p>

<p>The framework of policy representation learning proposed in our work is shown below. Policy network parameters used for <em>Origin Policy Representation (OPR)</em> or policy state-action pairs used for <em>Surface Policy Representation (SPR)</em> are fed into policy encoder with permutation-invariant (PI) transformations followed by an MLP, producing the representation $\chi_\pi$.</p>

<p>Afterwards, $\chi_{\pi}$ can be trained by gradients from the value approximation loss of PeVFA (i.e., End-to-End), as well as the auxiliary loss of policy recovery or contrastive learning (i.e., InfoNCE) loss.</p>

<p><img src="/assets/image/research/PeVFA/Untitled 3.png" alt="Untitled" /></p>

<h1 id="experiments">Experiments</h1>

<p>According to the paradigm of GPI with PeVFA and the general algorithm shown above, we replace the value network of vanilla PPO with PeVFA and obtain PPO-PeVFA. Experiments are conducted in OpenAI Gym continuous control tasks.</p>

<p>The aggregated performance of considered algorithms for comparative evaluation is shown below.</p>

<p><img src="/assets/image/research/PeVFA/Untitled 4.png" alt="Untitled" /></p>

<h3 id="visual-analysis">Visual Analysis</h3>

<p>The 2D t-SNE visualization of learned policy representation is shown in the figure below. We can observe that policies from different trials are locally continuous and show different modes of embedding trajectories due to random initialization and optimization; while a global evolvement among trials emerges with respect to policy performance.</p>

<p><img src="/assets/image/research/PeVFA/Untitled 5.png" alt="Untitled" /></p>]]></content><author><name></name></author><summary type="html"><![CDATA[We study Policy-extended Value Function Approximator (PeVFA) in Reinforcement Learning (RL), which extends conventional value function approximator (VFA) to take as input not only the state (and action) but also an explicit policy representation. Such an extension enables PeVFA to preserve values of multiple policies at the same time and brings an appealing characteristic, i.e., *value generalization among policies*.]]></summary></entry><entry><title type="html">ERL-Re2: Efficient Evolutionary Reinforcement Learning with Shared State Representation and Individual Policy Representation</title><link href="/2023/09/10/Re2.html" rel="alternate" type="text/html" title="ERL-Re2: Efficient Evolutionary Reinforcement Learning with Shared State Representation and Individual Policy Representation" /><published>2023-09-10T00:00:00+08:00</published><updated>2023-09-10T00:00:00+08:00</updated><id>/2023/09/10/Re2</id><content type="html" xml:base="/2023/09/10/Re2.html"><![CDATA[<p>æœ¬æ¬¡ä»‹ç»çš„æ˜¯æ–°çš„è¿›åŒ–å¼ºåŒ–å­¦ä¹ èŒƒå¼<strong>ERL-Re$^2$</strong>ã€‚è¯¥èŒƒå¼å……åˆ†èåˆäº†è¿›åŒ–ç®—æ³•ä¸å¼ºåŒ–å­¦ä¹ ç”¨äºç­–ç•¥ä¼˜åŒ–ï¼Œå¹¶å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½å¢ç›Šä¸æ•ˆæœã€‚</p>

<p>è¿›åŒ–ç®—æ³•ä¸ä¸å¼ºåŒ–å­¦ä¹ æ˜¯ä¸¤ç±»ä¸åŒçš„ä¼˜åŒ–æ–¹å¼ï¼Œæ“…é•¿è§£å†³ä¸åŒçš„ä¼˜åŒ–é—®é¢˜ï¼Œå¹¶ä¸”éƒ½æ‹¥æœ‰å¾ˆå¤§ï¼Œå¾ˆæ´»è·ƒçš„ç¤¾åŒºï¼Œæœ¬æ¬¡ä»‹ç»çš„ICLR2023çš„å·¥ä½œå°±æ˜¯ä¸ºäº†å°†ä¸¤ä¸ªç¤¾åŒºè¿æ¥èµ·æ¥ï¼Œå……åˆ†åˆ©ç”¨ä¸¤ç§ä¸åŒä¼˜åŒ–ç®—æ³•å„è‡ªçš„ä¼˜åŠ¿æ¥å®ç°ç­–ç•¥æœç´¢ä¸æ€§èƒ½æå‡ ã€‚ç›®å‰ä»£ç å·²ç»å¼€æºã€‚</p>

<h2 id="background">Background</h2>

<p><strong>å¼ºåŒ–å­¦ä¹  Reinforcement Learning</strong>ï¼ˆRLï¼‰å¯ä»¥é€šè¿‡ç¯å¢ƒè¯•é”™å’Œæ¢¯åº¦æ›´æ–°æ¥é«˜æ•ˆåœ°å­¦ä¹ ã€‚ç„¶è€Œï¼Œä¼—æ‰€å‘¨çŸ¥ï¼ŒRLé²æ£’æ€§å·®ï¼Œæ¢ç´¢æ€§å·®ï¼Œå¹¶ä¸”åœ¨æ¢¯åº¦ä¿¡å·æœ‰å™ªå£°å’Œä¿¡æ¯é‡è¾ƒå°‘ï¼ˆsparseï¼‰çš„æƒ…å†µä¸‹ï¼Œéš¾ä»¥é«˜æ•ˆè®­ç»ƒã€‚
<strong>è¿›åŒ–ç®—æ³• Evolutionary Algorithms</strong>ï¼ˆEAï¼‰æ˜¯ä¸€ç±»é»‘ç®±ä¼˜åŒ–æ–¹æ³•ï¼Œä¸»è¦æ˜¯ç»´æŠ¤ä¸€ä¸ªä¸ªä½“çš„ç§ç¾¤ï¼Œè€Œä¸æ˜¯åƒRLåªç»´æŠ¤ä¸€ä¸ªä¸ªä½“ï¼Œé€šè¿‡éšæœºæ‰°åŠ¨çš„æ–¹å¼æ¥æå‡ä¸ªä½“è·å¾—å¯è¡Œè§£ã€‚ä¸RLä¸åŒçš„æ˜¯ï¼Œä¼ ç»ŸEAæ˜¯æ— æ¢¯åº¦ä¼˜åŒ–æ–¹æ³•ï¼Œå¹¶å…·æœ‰å‡ ä¸ªä¼˜ç‚¹ï¼š
ï¼ˆ1ï¼‰å¼ºå¤§çš„æ¢ç´¢èƒ½åŠ›ã€ï¼ˆ2ï¼‰é²æ£’æ€§å’Œç¨³å®šçš„æ”¶æ•›ã€ï¼ˆ3ï¼‰é‡‡ç”¨ç´¯è®¡å¥–åŠ±è¯„ä»·ä¸ªä½“ï¼Œä¸å…³å¿ƒå•æ­¥å¥–åŠ±ï¼Œå› æ­¤å¯¹å¥–åŠ±ä¿¡å·ä¸æ•æ„Ÿã€‚
å°½ç®¡æœ‰è¿™äº›ä¼˜ç‚¹ï¼ŒEAçš„ä¸€ä¸ªä¸»è¦ç“¶é¢ˆæ˜¯ç¾¤ä½“çš„è¿­ä»£è¯„ä¼°è€Œå¯¼è‡´çš„ä½æ ·æœ¬æ•ˆç‡ã€‚å…·ä½“æ¥è¯´ï¼ŒEAéœ€è¦ç§ç¾¤ä¸­çš„æ¯ä¸ªä¸ªä½“ä¸ç¯å¢ƒçœŸå®äº¤äº’æ¥è·å¾—é€‚åº”åº¦ï¼ˆæ€§èƒ½è¡¨ç°ï¼‰ï¼Œæœ€ç»ˆæ ¹æ®ç§ç¾¤ä¸­ä¸åŒä¸ªä½“çš„é€‚åº”åº¦æ¥è¿›è¡Œç§ç¾¤æå‡ã€‚</p>

<p>å¾ˆå¤šå·¥ä½œéƒ½åœ¨ç ”ç©¶å¦‚ä½•å°†EAå’ŒRLçš„èåˆèµ·æ¥ï¼Œå–é•¿è¡¥çŸ­ï¼Œä¼˜åŠ¿äº’è¡¥ã€‚å…¶ä¸­æœ€å…·æœ‰ä»£è¡¨æ€§çš„å½“å±2018å¹´æå‡ºçš„æ¼”åŒ–å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼ˆERLï¼‰ï¼Œå°†Genetic Algorithmï¼ˆGAï¼‰ä¸DDPGè¿›è¡Œäº†èåˆã€‚é™¤äº†ç»´æŠ¤å¼ºåŒ–å­¦ä¹ çš„actorå’Œcriticï¼ŒERLé¢å¤–ç»´æŠ¤ä¸€ä¸ªçš„actorçš„ç§ç¾¤ã€‚ä¸ºäº†èåˆåŒæ–¹çš„ä¼˜ç‚¹ï¼ŒEAä¸ç¯å¢ƒäº¤äº’äº§ç”Ÿçš„å¤šæ ·æ€§çš„æ ·æœ¬ä¼šæä¾›ç»™RLç”¨äºoff policyä¼˜åŒ–ï¼Œè¿™ä¸€æ–¹é¢è§£å†³äº†EAæ ·æœ¬åˆ©ç”¨ç‡ä½çš„é—®é¢˜ï¼Œå¦ä¸€æ–¹é¢ç¼“è§£äº†RLæ¢ç´¢å¼±æ— æ³•å¯»æ‰¾åˆ°å¤šæ ·æ•°æ®çš„é—®é¢˜ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œä¼˜åŒ–åçš„RL ç­–ç•¥ä¼šå®šæœŸæ³¨å…¥åˆ°ç§ç¾¤ä¸­å‚ä¸ç§ç¾¤è¿›åŒ–ï¼Œå¦‚æœRLç­–ç•¥ä¼˜äºç§ç¾¤ç­–ç•¥ï¼Œé‚£ä¹ˆRLç­–ç•¥åˆ™ä¼šä¿ƒè¿›ç§ç¾¤çš„è¿›åŒ–ï¼Œå¦åˆ™åˆ™ä¼šè¢«æ·˜æ±°æ‰ï¼Œä¸å½±å“ç§ç¾¤è¿›åŒ–ã€‚æœ€ç»ˆEAä¸RLä¼˜åŠ¿äº’è¡¥ï¼Œåœ¨MuJoCoä¸Šå®ç°äº†å¯¹DDPGç®—æ³•çš„æ˜¾è‘—æå‡ã€‚ï¼ˆè¿™é‡Œçš„EAæ¼”åŒ–éƒ½æ˜¯ç›´æ¥åœ¨ç­–ç•¥çš„å‚æ•°ä¸Šè¿›è¡Œæ‰°åŠ¨ä¼˜åŒ–ï¼Œä¾‹å¦‚kç‚¹äº¤å‰æ˜¯äº¤æ¢ä¸¤ä¸ªç½‘ç»œä¸­æŸäº›å±‚çš„å‚æ•°ï¼Œå˜å¼‚åˆ™ç›´æ¥å°†é«˜æ–¯æ‰°åŠ¨æ·»åŠ åˆ°ç½‘ç»œå‚æ•°ä¸Šï¼‰</p>

<h2 id="motivation">Motivation</h2>

<p>ERLå·¥ä½œåï¼Œè®¸å¤šåŸºäºERLåŸºæœ¬æ¡†æ¶çš„ç›¸å…³å·¥ä½œéšä¹‹äº§ç”Ÿï¼Œä¾‹å¦‚CERLï¼ŒPDERL, CEM-RLç­‰ã€‚ç”±äºéƒ½éµå¾ªåŸºæœ¬çš„ERLæ¡†æ¶ï¼Œå¯¼è‡´è¿™äº›ç®—æ³•éƒ½é¢ä¸´ç€ä¸¤ä¸ªåŸºæœ¬é—®é¢˜ï¼š</p>

<ul>
  <li>æ‰€æœ‰çš„ç­–ç•¥éƒ½å•ç‹¬å­¦ä¹ è‡ªå·±çš„çŠ¶æ€è¡¨å¾ï¼Œç»´æŠ¤å„è‡ªçš„ç½‘ç»œï¼Œå¿½ç•¥äº†æœ‰æ•ˆçŸ¥è¯†çš„å…±äº«ã€‚</li>
  <li>å¯¹äºæ¼”åŒ–ç®—å­ï¼Œå‚æ•°å±‚é¢çš„ç­–ç•¥ä¼˜åŒ–ä¸èƒ½ä¿è¯ä¸ªä½“çš„è¡Œä¸ºè¯­ä¹‰ï¼Œå®¹æ˜“é€ æˆç­–ç•¥ç¾éš¾æ€§å´©æºƒã€‚</li>
</ul>

<h2 id="the-concept-of-two-scale-state-representation-and-policy-representation">The Concept of Two-Scale State Representation and Policy Representation</h2>
<p>ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†<strong>åŸºäºåŒå°ºåº¦è¡¨å¾çš„ç­–ç•¥æ„å»º</strong>ï¼ˆ<strong>Two-scale representation-based policy construction</strong>ï¼‰ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬ç»´æŠ¤å’Œä¼˜åŒ–EAç¾¤ä½“å’ŒRLçš„ç­–ç•¥ã€‚å…·ä½“æ¥è¯´ã€‚EAå’ŒRL Agentçš„ç­–ç•¥éƒ½æ˜¯ç”±ä¸€ä¸ªå…±äº«çš„éçº¿æ€§çŠ¶æ€è¡¨å¾$Z{\phi}$å’Œä¸€ä¸ªç‹¬ç«‹çš„çº¿æ€§ç­–ç•¥è¡¨å¾ $W$ ç»„æˆã€‚Agent $i$ é€šè¿‡ç»“åˆå…±äº«çŠ¶æ€è¡¨å¾å’Œç­–ç•¥è¡¨å¾åšå‡ºå†³ç­–ï¼š</p>

<p><img src="/assets/image/research/Re2/1.png" alt="1" /></p>

<p>ç›´è§‚åœ°ï¼Œæˆ‘ä»¬å¸Œæœ›å…±äº«çŠ¶æ€è¡¨å¾$Z{\phi}$å¯¹å­¦ä¹ è¿‡ç¨‹ä¸­é‡åˆ°çš„æ‰€æœ‰å¯èƒ½çš„ç­–ç•¥éƒ½æœ‰ç”¨ã€‚å®ƒåº”è¯¥åŒ…å«ç¯å¢ƒä¸­ä¸å†³ç­–æœ‰å…³çš„ä¸€èˆ¬ç‰¹å¾ï¼Œä¾‹å¦‚ï¼Œå…±åŒçš„çŸ¥è¯†ï¼Œè€Œä¸æ˜¯é’ˆå¯¹æŸä¸€ä¸ªç­–ç•¥ã€‚ç”±äºå…±äº«çŠ¶æ€è¡¨ç¤º$Z_{\phi}$ï¼ŒAgentä¸éœ€è¦ç‹¬ç«‹åœ°è¡¨å¾çŠ¶æ€ã€‚å› æ­¤ï¼Œæ›´é«˜çš„æ•ˆç‡å’Œæ›´å…·è¡¨ç°åŠ›çš„çŠ¶æ€è¡¨å¾å¯ä»¥é€šè¿‡EAç¾¤ä½“å’ŒRL Agentå…±åŒå¾—åˆ°ã€‚ç”±äº$Z{\phi}$çš„é«˜è¡¨è¾¾æ€§ï¼Œæ¯ä¸ªç‹¬ç«‹çš„ç­–ç•¥è¡¨å¾å¯ä»¥ç”±ä¸€ä¸ªç®€å•çš„çº¿æ€§å½¢å¼æ„æˆï¼Œè¿™æ›´æ˜“äºä¼˜åŒ–ä¸æ¢ç´¢ã€‚</p>

<p><img src="/assets/image/research/Re2/2.png" alt="6a224755e7478c57e45d7a91c514a23c.png" /></p>

<p>ä¸Šå›¾æ˜¯ERLï¼ˆåŠåç»­å·¥ä½œï¼‰ä¸æˆ‘ä»¬æå‡ºçš„åŒå°ºåº¦è¡¨å¾æ¡†æ¶ERL-ReÂ²çš„å¯¹æ¯”å›¾ã€‚å…¶ä¸­å·¦å›¾ä¸­çš„ç­–ç•¥ä¸»è¦ç”±ä¼ ç»Ÿçš„éçº¿æ€§ç¥ç»ç½‘ç»œæ„æˆã€‚å³å›¾ä¸­çš„åœ†å½¢è¡¨ç¤ºçº¿æ€§ç­–ç•¥è¡¨å¾ï¼Œå…­è¾¹å½¢åˆ™è¡¨ç¤ºéçº¿æ€§å…±äº«çŠ¶æ€è¡¨å¾ï¼Œç”¨äºçŸ¥è¯†å…±äº«ã€‚*</p>

<p><em><img src="/assets/image/research/Re2/3.png" alt="3" /></em></p>

<p><strong>ç®—æ³•ä¼˜åŒ–æµç¨‹</strong>:æ•´ä½“ä¼˜åŒ–æµç¨‹å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œå…·ä½“æ¥è¯´ï¼Œç®—æ³•æ¯æ¬¡åœ¨ç”±å…±äº«çŠ¶æ€è¡¨å¾$Z{\phi}$æ„å»ºçš„çº¿æ€§ç­–ç•¥ç©ºé—´ $\Pi{\phi}$ ä¸­è¿›è¡Œç­–ç•¥æœç´¢ï¼Œå¯¹çº¿æ€§ç­–ç•¥è¿›è¡Œä¼˜åŒ–ã€‚ä¼˜åŒ–åæˆ‘ä»¬å¯¹å…±äº«çŠ¶æ€è¡¨å¾è¿›è¡Œä¼˜åŒ–ï¼Œä¼˜åŒ–çš„æ–¹å‘ä¸ºå¯¹äºæ‰€æœ‰ä¸ªä½“ï¼ˆåŒ…æ‹¬EAå’ŒRL)éƒ½æœ‰ç›Šçš„æ–¹å‘ï¼Œä»è€Œè¾¾åˆ°æœ‰æ•ˆçš„çŸ¥è¯†å…±äº«ï¼Œæ„å»ºå¯¹äºæ‰€æœ‰ä¸ªä½“éƒ½æœ‰åˆ©çš„ç­–ç•¥ç©ºé—´ã€‚å¦‚æ­¤å¾ªç¯è¿­ä»£å®ç°çŸ¥è¯†çš„é«˜æ•ˆä¼ é€’ä¸ç­–ç•¥çš„å¿«é€Ÿä¼˜åŒ–ã€‚ä¸‹é¢æˆ‘ä»¬ä»‹ç»å¦‚ä½•è¿›è¡Œå…±äº«è¡¨å¾çš„ä¼˜åŒ–ï¼Œä»¥åŠå¦‚ä½•åœ¨çº¿æ€§ç©ºé—´å¦‚ä½•æ›´åŠ é«˜æ•ˆåœ°æ¼”åŒ–ã€‚</p>

<h2 id="optimizing-the-shared-state-representation-for-a-superior-policy-space"><em>Optimizing the Shared State Representation for A Superior Policy Space</em></h2>
<p>ä¸ºäº†æ„å»ºæ‰€æœ‰ä¸ªä½“éƒ½æœ‰ç›Šçš„çŠ¶æ€è¡¨å¾ä»è€Œå®ç°é«˜æ•ˆåœ°çŸ¥è¯†å…±äº«ï¼Œæˆ‘ä»¬æå‡ºåŸºäºæ‰€æœ‰EAå’ŒRLç­–ç•¥çš„ä»·å€¼å‡½æ•°æœ€å¤§åŒ–æ¥å­¦ä¹ å…±äº«çŠ¶æ€è¡¨å¾ã€‚å¯¹äºEAç­–ç•¥ï¼Œæˆ‘ä»¬æ ¹æ®EAç¾¤ä½“â„™ä¸­çš„çº¿æ€§ç­–ç•¥è¡¨ç¤º ï¼Œå­¦ä¹ ç­–ç•¥æ‹“å±•å€¼å‡½æ•°ï¼ˆPeVFA, é€šè¿‡å°†ç­–ç•¥è¡¨å¾ä½œä¸ºè¾“å…¥ï¼Œå®ç°ä¸€ä¸ªä»·å€¼å‡½æ•°ä¼°è®¡å¤šä¸ªä¸åŒç­–ç•¥valueçš„ç›®çš„ï¼‰ã€‚å¯¹äºRLç­–ç•¥ï¼Œæˆ‘ä»¬ä½¿ç”¨åŸå§‹RLç®—æ³•çš„å€¼å‡½æ•°æä¾›æ›´æ–°æ–¹å‘ã€‚ä¸¤ä¸ªå€¼å‡½æ•°éƒ½æ˜¯é€šè¿‡TD errorè¿›è¡Œä¼˜åŒ–çš„ï¼ŒæŸå¤±å¦‚ä¸‹ï¼š</p>

<p><img src="/assets/image/research/Re2/4.png" alt="4" /></p>

<p>EAä¸­çš„ä¸ªä½“å’ŒRLä¸ªä½“éƒ½èƒ½åˆ†åˆ«ä»PeVFAå’ŒRL criticè·å¾—å„è‡ªçš„ä¼˜åŒ–æ–¹å‘. è€Œæˆ‘ä»¬æƒ³æ„å»ºçš„å…±äº«çŠ¶æ€è¡¨å¾åº”è¯¥æœ‰åŠ©äºæ‰€æœ‰ä¸ªä½“çš„æ¢ç´¢ä¸ä¼˜åŒ–ï¼Œå› æ­¤å…±äº«è¡¨å¾çš„æ›´æ–°æ–¹å‘åº”è¯¥è€ƒè™‘åˆ°EAå’ŒRLï¼Œå› æ­¤æˆ‘ä»¬å®šä¹‰äº†å¦‚ä¸‹æŸå¤±ï¼š</p>

<p><img src="/assets/image/research/Re2/5.png" alt="5" /><br />
é€šè¿‡ä¼˜åŒ–ä¸Šè¿°æŸå¤±ï¼Œå…±äº«çŠ¶æ€è¡¨å¾èƒ½å¤Ÿå‘ç€ä¸€ä¸ªç»Ÿä¸€çš„ä¼˜åŒ–æ–¹å‘è¿›è¡Œä¼˜åŒ–ï¼Œä»è€Œæ„å»ºä¸€ä¸ªæœ‰åŠ©äºæ‰€æœ‰ä¸ªä½“çš„çº¿æ€§ç­–ç•¥ç©ºé—´ï¼Œä½¿å¾—EAå’ŒRLèƒ½å¤Ÿæ›´åŠ é«˜æ•ˆåœ°æ¢ç´¢ä¸æå‡ã€‚</p>

<h2 id="optimizing-the-policy-representation-by-evolution-and-reinforcement">Optimizing the Policy Representation by Evolution and Reinforcement</h2>
<p>å¯¹äºç§ç¾¤çš„è¿›åŒ–ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦å¾—åˆ°é€‚é…åº¦ï¼ˆfitnessï¼‰ï¼Œæ‰€äº§ç”Ÿçš„æ ·æœ¬å¼€é”€æ˜¯EAçš„ä¸€ä¸ªä¸»è¦ç“¶é¢ˆï¼Œç‰¹åˆ«æ˜¯å½“ç§ç¾¤å¾ˆå¤§æ—¶ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºPeVFA çš„æ–°çš„é€‚åº”åº¦å‡½æ•°ã€‚å¯¹äºæ¯ä¸ªAgent ï¼Œæˆ‘ä»¬è®©Agentä¸ç¯å¢ƒäº¤äº’$ğ»$æ­¥ï¼Œéšåä½¿ç”¨PeVFAè¿›è¡Œä¼°å€¼æ¥èŠ‚çœæ ·æœ¬å¼€é”€ã€‚ fitnessè¢«å®šä¹‰å¦‚ä¸‹ï¼š</p>

<p><img src="/assets/image/research/Re2/6.png" alt="6" /></p>

<p>å¯¹äºé—ä¼ è¿›åŒ–çš„è¿‡ç¨‹ï¼Œä¼ ç»Ÿçš„äº¤å‰å˜å¼‚éƒ½æ˜¯ç›´æ¥åœ¨æ•´ä¸ªç­–ç•¥çš„å‚æ•°ç©ºé—´è¿›è¡Œæ‰°åŠ¨ï¼Œç”±äºç­–ç•¥å¾€å¾€æ˜¯ç”±éçº¿æ€§çš„ç¥ç»ç½‘ç»œæ„å»ºçš„ã€‚å•ç‹¬çš„æ”¹å˜ç¥ç»ç½‘ç»œçš„æŸäº›å‚æ•°å¯èƒ½ä¼šé€ æˆç­–ç•¥è¡Œä¸ºçš„åå¡Œä¸å´©æºƒã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ–°çš„behavior-level äº¤å‰å’Œå˜å¼‚ï¼Œå…è®¸åœ¨æŒ‡å®šçš„è¡ŒåŠ¨ç»´åº¦ä¸Šæ–½åŠ æ‰°åŠ¨ï¼ŒåŒæ—¶å¯¹å…¶ä»–åŠ¨ä½œä¸äº§ç”Ÿä»»ä½•å¹²æ‰°ã€‚å…·ä½“æ¥è¯´ï¼Œç”±äºå…±äº«çŠ¶æ€è¡¨å¾çš„æ„å»ºï¼Œæ¼”åŒ–å‘ç”Ÿåœ¨çº¿æ€§ç­–ç•¥è¡¨å¾ç©ºé—´ï¼Œçº¿æ€§ç­–ç•¥è¡¨å¾çš„æ¯ä¸ªç»´åº¦å¯¹åº”å†³ç­–çš„ä¸€ä¸ªåŠ¨ä½œï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥ç›´æ¥äº¤æ¢è¡¨å¾çš„æŸä¸€ç»´åº¦çš„å‚æ•°ï¼Œè€Œå®ç°ä¸¤ä¸ªç­–ç•¥çš„æŸä¸ªåŠ¨ä½œçš„äº¤å‰ï¼Œè€Œä¸å¯¹å…¶ä»–åŠ¨ä½œäº§ç”Ÿæ‰°åŠ¨ï¼ˆbehavior-level crossoverï¼‰ï¼ŒåŒæ ·æ‰°åŠ¨ä¹Ÿå¯ä»¥è¢«å•ç‹¬åŠ åœ¨è¡¨å¾çš„æŸä¸ªç‰¹å®šç»´åº¦ä¸Šï¼Œä¸å¯¹å…¶ä»–åŠ¨ä½œäº§ç”Ÿæ‰°åŠ¨ï¼ˆbehavior-level mutationï¼‰ã€‚behavior-level äº¤å‰å˜å¼‚çš„ç¤ºæ„å›¾å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚</p>

<p><img src="/assets/image/research/Re2/7.png" alt="7" /></p>

<p>æœ€åæ•´ä¸ªå®éªŒçš„ä¼ªä»£ç ï¼š</p>

<p><img src="/assets/image/research/Re2/8.png" alt="8" /></p>

<h2 id="experiments">Experiments</h2>
<p>æœ¬æ–‡å®éªŒä¸»è¦åœ¨MUJOCOçš„6ä¸ªå¸¸ç”¨çš„taskä¸ŠéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼ŒåŸºæœ¬ä¸Šéƒ½æœ‰å¤§å¹…åº¦çš„æ€§èƒ½å¢ç›Šï¼Œè¾¾åˆ°äº†åœ¨è¿™ä¸ªbenchmarkä¸Šçš„ERLæ–¹å‘çš„æ–°SOTAã€‚é™¤æ­¤ä¹‹å¤–ï¼Œæœ¬æ–‡ä¹Ÿå°è¯•äº†ä¸€äº›å…¶ä»–ç¯å¢ƒå’Œç®—æ³•ï¼Œå¤§å®¶å¯ä»¥åˆ°åŸæ–‡ä¸­æŸ¥çœ‹æ›´å¤šçš„ç»†èŠ‚ã€‚</p>

<p><em><img src="/assets/image/research/Re2/10.png" alt="10" /><img src="/assets/image/research/Re2/9.png" alt="9" /></em></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Deep Reinforcement Learning (Deep RL) and Evolutionary Algorithms (EA) are two major paradigms of policy optimization with distinct learning principles, i.e., gradient-based v.s. gradient-free. An appealing research direction is integrating Deep RL and EA to devise new methods by fusing their complementary advantages. However, existing works on combining Deep RL and EA have two common drawbacks: 1) the RL agent and EA agents learn their policies individually, neglecting efficient sharing of useful common knowledge; 2) parameter-level policy optimization guarantees no semantic level of behavior evolution for the EA side. In this paper, we propose Evolutionary Reinforcement Learning with Two-scale State Representation and Policy Representation (ERL-Re^2), a novel solution to the aforementioned two drawbacks. The key idea of ERL-Re^2 is two-scale representation: all EA and RL policies share the same nonlinear state representation while maintaining individual} linear policy representations. The state representation conveys expressive common features of the environment learned by all the agents collectively; the linear policy representation provides a favorable space for efficient policy optimization, where novel behavior-level crossover and mutation operations can be performed. Moreover, the linear policy representation allows convenient generalization of policy fitness with the help of the Policy-extended Value Function Approximator (PeVFA), further improving the sample efficiency of fitness estimation. The experiments on a range of continuous control tasks show that ERL-Re^2 consistently outperforms advanced baselines and achieves the State Of The Art (SOTA).]]></summary></entry><entry><title type="html">Boosting Multiagent Reinforcement Learning via Permutation Invariant and Permutation Equivariant Networks</title><link href="/2023/09/09/API.html" rel="alternate" type="text/html" title="Boosting Multiagent Reinforcement Learning via Permutation Invariant and Permutation Equivariant Networks" /><published>2023-09-09T00:00:00+08:00</published><updated>2023-09-09T00:00:00+08:00</updated><id>/2023/09/09/API</id><content type="html" xml:base="/2023/09/09/API.html"><![CDATA[<script>
    window.addEventListener('load', function() {
        window.location.href = 'https://github.com/tjuHaoXiaotian/pymarl3';
    });
</script>]]></content><author><name></name></author><summary type="html"><![CDATA[The state space in Multiagent Reinforcement Learning (MARL) grows exponentially with the agent number. Such a curse of dimensionality results in poor scalability and low sample efficiency, inhibiting MARL for decades. To break this curse, we propose a unified agent permutation framework that exploits the permutation invariance (PI) and permutation equivariance (PE) inductive biases to reduce the multiagent state space. Our insight is that permuting the order of entities in the factored multiagent state space does not change the information.]]></summary></entry><entry><title type="html">MetaDiffuser: Diffusion Model as Conditional Planner for Offline Meta-RL</title><link href="/2023/08/01/MetaDiffuser.html" rel="alternate" type="text/html" title="MetaDiffuser: Diffusion Model as Conditional Planner for Offline Meta-RL" /><published>2023-08-01T00:00:00+08:00</published><updated>2023-08-01T00:00:00+08:00</updated><id>/2023/08/01/MetaDiffuser</id><content type="html" xml:base="/2023/08/01/MetaDiffuser.html"><![CDATA[<script>
    window.addEventListener('load', function() {
        window.location.href = 'https://metadiffuser.github.io/';
    });
</script>]]></content><author><name></name></author><summary type="html"><![CDATA[Recently, diffusion model shines as a promising backbone for the sequence modeling paradigm in offline reinforcement learning. However, these works mostly lack the generalization ability across tasks with reward or dynamics change. To tackle this challenge, in this paper we propose a task-oriented conditioned diffusion planner for offline meta-RL(MetaDiffuser), which considers the generalization problem as conditional trajectory generation task with contextual representation. The key is to learn a context conditioned diffusion model which can generate task-oriented trajectories for planning across diverse tasks. To enhance the dynamics consistency of the generated trajectories while encouraging trajectories to achieve high returns, we further design a dual-guided module in the sampling process of the diffusion model. The proposed framework enjoys the robustness to the quality of collected warm-start data from the testing task and the flexibility to incorporate with different task representation method. The experiment results on MuJoCo benchmarks show that MetaDiffuser outperforms other strong offline meta-RL baselines, demonstrating the outstanding conditional generation ability of diffusion architecture.]]></summary></entry></feed>